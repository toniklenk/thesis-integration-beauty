{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classes\n",
    "import caffemodel2pytorch as caffe\n",
    "\n",
    "# modified visualpriors library\n",
    "from transforms import VisualPriorRepresentation, VisualPriorPredictedLabel\n",
    "from taskonomy_network import TaskonomyEncoder, TaskonomyDecoder\n",
    "\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchsummary\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.io import savemat\n",
    "\n",
    "import torchvision.io\n",
    "import torch.utils.model_zoo # required to load nets\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch import nn\n",
    "import torchvision.models\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To check that the implementation of the code is correct, the analysis of vgg16_playes net is repeated here,\n",
    "which should reproduce the same results as in the matlab implementation.\n",
    "\n",
    "The boilerplate code for this is a bit different, since the vgg16_places model is imported from a different source\n",
    "and in a different format than the Taskonomy models.\n",
    "\n",
    "As soon as the model is brought into the right format, the analysis steps are the same in principle.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to import model into replicated architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU1_1): ReLU()\n",
       "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU1_2): ReLU()\n",
       "  (MaxPool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU2_1): ReLU()\n",
       "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU2_2): ReLU()\n",
       "  (MaxPool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU3_1): ReLU()\n",
       "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU3_2): ReLU()\n",
       "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU3_3): ReLU()\n",
       "  (MaxPool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU4_1): ReLU()\n",
       "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU4_2): ReLU()\n",
       "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU4_3): ReLU()\n",
       "  (MaxPool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU5_1): ReLU()\n",
       "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU5_2): ReLU()\n",
       "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU5_3): ReLU()\n",
       "  (MaxPool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc6): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (ReLU6): ReLU()\n",
       "  (fc7): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (ReLU7): ReLU()\n",
       "  (fc8a): Linear(in_features=4096, out_features=365, bias=True)\n",
       "  (Softmax8a): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_places = nn.Sequential(\n",
    "    collections.OrderedDict(\n",
    "        [\n",
    "            ('conv1_1', nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU1_1', nn.ReLU()),\n",
    "            ('conv1_2', nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU1_2',nn.ReLU()),\n",
    "            ('MaxPool1', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "            ('conv2_1',nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU2_1',nn.ReLU()),\n",
    "            ('conv2_2',nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU2_2',nn.ReLU()),\n",
    "            ('MaxPool2', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "            ('conv3_1', nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU3_1',nn.ReLU()),\n",
    "            ('conv3_2', nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU3_2',nn.ReLU()),\n",
    "            ('conv3_3', nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU3_3',nn.ReLU()),\n",
    "            ('MaxPool3', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "            ('conv4_1', nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU4_1',nn.ReLU()),\n",
    "            ('conv4_2', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU4_2',nn.ReLU()),\n",
    "            ('conv4_3', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU4_3',nn.ReLU()),\n",
    "            ('MaxPool4', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "            ('conv5_1', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU5_1',nn.ReLU()),\n",
    "            ('conv5_2', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU5_2',nn.ReLU()),\n",
    "            ('conv5_3', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU5_3',nn.ReLU()),\n",
    "            ('MaxPool5', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "            #('Flatten6', nn.Flatten()),\n",
    "            ('fc6', nn.Linear(in_features=25088, out_features=4096)),\n",
    "            ('ReLU6',nn.ReLU()),\n",
    "            ('fc7', nn.Linear(in_features=4096, out_features=4096)),\n",
    "            ('ReLU7',nn.ReLU()),\n",
    "            ('fc8a', nn.Linear(in_features=4096, out_features=365)),\n",
    "            ('Softmax8a', nn.Softmax(dim=-1))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "vgg16_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_places_statedict = torch.load('vgg16_places365.caffemodel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_places.load_state_dict(vgg16_places_statedict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input   conv1   ReLU1   conv1_1   ReLU1_1   MaxPool1   conv2   ReLU2   conv2_1   ReLU2_1   MaxPool2   conv3   ReLU3   conv3_1   ReLU3_1   conv3_2   ReLU3_2   MaxPool3   conv4   ReLU4   conv4_1   ReLU4_1   conv4_2   ReLU4_2   MaxPool4   conv5   ReLU5   conv5_1   ReLU5_1   conv5_2   ReLU5_2   MaxPool5   fc6   ReLU6   fc7   ReLU7   fc8a   Softmax8a   "
     ]
    }
   ],
   "source": [
    "_, eval_nodes = get_graph_node_names(vgg16_places)\n",
    "for node in eval_nodes:\n",
    "    print(node, end='   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_nodes = { node:node for node in eval_nodes if \"conv\" in node or 'fc' in node}\n",
    "\n",
    "vgg16_places_fe = create_feature_extractor(vgg16_places, return_nodes=return_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = torchvision.io.read_image('./data/stimuli_places1/Places365_val_00036011.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_images = classes.ImageDataset('./data/stimuli_places1', beauty_ratings_path='./behavior/ratings_study1.csv')\n",
    "integration_calc = classes.IntegrationCalculator(vgg16_places_fe, return_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10240x20 and 25088x4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclasses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_dataset_integration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplaces_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintegration_calc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Thesis/Taskonomy Integration/classes.py:123\u001b[0m, in \u001b[0;36mcalculate_dataset_integration\u001b[0;34m(dataset_iterator, integration_calculator)\u001b[0m\n\u001b[1;32m    121\u001b[0m lst \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m dataset_iterator:\n\u001b[0;32m--> 123\u001b[0m     lst\u001b[38;5;241m.\u001b[39mappend(\u001b[43mintegration_calculator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegration_coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(lst, columns\u001b[38;5;241m=\u001b[39mintegration_calculator\u001b[38;5;241m.\u001b[39mevalutation_layers)\n",
      "File \u001b[0;32m~/Documents/Thesis/Taskonomy Integration/classes.py:87\u001b[0m, in \u001b[0;36mIntegrationCalculator.integration_coeff\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     83\u001b[0m img, img1, img2 \u001b[38;5;241m=\u001b[39m TF\u001b[38;5;241m.\u001b[39mto_tensor(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), TF\u001b[38;5;241m.\u001b[39mto_tensor(img1)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), TF\u001b[38;5;241m.\u001b[39mto_tensor(img2)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# activation for full image\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m img_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# average activation of image parts\u001b[39;00m\n\u001b[1;32m     90\u001b[0m img1_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(img1)\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/fx/graph_module.py:738\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/fx/graph_module.py:317\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# noqa: TRY200\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/fx/graph_module.py:304\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m<eval_with_key>.10 from <eval_with_key>.4 from /home/max/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/nn/modules/container.py:215 in forward:4 in forward:37\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m re_lu5_3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU5_3(conv5_3)\n\u001b[1;32m     36\u001b[0m max_pool5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMaxPool5(re_lu5_3);  re_lu5_3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m fc6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_pool5\u001b[49m\u001b[43m)\u001b[49m;  max_pool5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     38\u001b[0m re_lu6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU6(fc6)\n\u001b[1;32m     39\u001b[0m fc7 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc7(re_lu6);  re_lu6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10240x20 and 25088x4096)"
     ]
    }
   ],
   "source": [
    "classes.calculate_dataset_integration(places_images, integration_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to replace Flatten Layer with a custom built reshape layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class class_vgg16_tweaked(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(class_vgg16_tweaked, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            collections.OrderedDict(\n",
    "                [\n",
    "                    ('conv1_1', nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU1_1', nn.ReLU()),\n",
    "                    ('conv1_2', nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU1_2',nn.ReLU()),\n",
    "                    ('MaxPool1', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "                    ('conv2_1',nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU2_1',nn.ReLU()),\n",
    "                    ('conv2_2',nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU2_2',nn.ReLU()),\n",
    "                    ('MaxPool2', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "                    ('conv3_1', nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU3_1',nn.ReLU()),\n",
    "                    ('conv3_2', nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU3_2',nn.ReLU()),\n",
    "                    ('conv3_3', nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU3_3',nn.ReLU()),\n",
    "                    ('MaxPool3', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "                    ('conv4_1', nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU4_1',nn.ReLU()),\n",
    "                    ('conv4_2', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU4_2',nn.ReLU()),\n",
    "                    ('conv4_3', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU4_3',nn.ReLU()),\n",
    "                    ('MaxPool4', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "                    ('conv5_1', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU5_1',nn.ReLU()),\n",
    "                    ('conv5_2', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU5_2',nn.ReLU()),\n",
    "                    ('conv5_3', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU5_3',nn.ReLU()),\n",
    "                    ('MaxPool5', nn.MaxPool2d(kernel_size=2, stride=2,padding=0))\n",
    "            ]))\n",
    "        self.fullyconnected_layers = nn.Sequential(\n",
    "            collections.OrderedDict(\n",
    "                [\n",
    "                    ('fc6', nn.Linear(in_features=25088, out_features=4096)),\n",
    "                    ('ReLU6',nn.ReLU()),\n",
    "                    ('fc7', nn.Linear(in_features=4096, out_features=4096)),\n",
    "                    ('ReLU7',nn.ReLU()),\n",
    "                    ('fc8a', nn.Linear(in_features=4096, out_features=365)),\n",
    "                    ('Softmax8a', nn.Softmax(dim=-1))\n",
    "            ]))\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fullyconnected_layers(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_tweaked = class_vgg16_tweaked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x',\n",
       " 'conv_layers.conv1',\n",
       " 'conv_layers.ReLU1',\n",
       " 'conv_layers.conv1_1',\n",
       " 'conv_layers.ReLU1_1',\n",
       " 'conv_layers.MaxPool1',\n",
       " 'conv_layers.conv2',\n",
       " 'conv_layers.ReLU2',\n",
       " 'conv_layers.conv2_1',\n",
       " 'conv_layers.ReLU2_1',\n",
       " 'conv_layers.MaxPool2',\n",
       " 'conv_layers.conv3',\n",
       " 'conv_layers.ReLU3',\n",
       " 'conv_layers.conv3_1',\n",
       " 'conv_layers.ReLU3_1',\n",
       " 'conv_layers.conv3_2',\n",
       " 'conv_layers.ReLU3_2',\n",
       " 'conv_layers.MaxPool3',\n",
       " 'conv_layers.conv4',\n",
       " 'conv_layers.ReLU4',\n",
       " 'conv_layers.conv4_1',\n",
       " 'conv_layers.ReLU4_1',\n",
       " 'conv_layers.conv4_2',\n",
       " 'conv_layers.ReLU4_2',\n",
       " 'conv_layers.MaxPool4',\n",
       " 'conv_layers.conv5',\n",
       " 'conv_layers.ReLU5',\n",
       " 'conv_layers.conv5_1',\n",
       " 'conv_layers.ReLU5_1',\n",
       " 'conv_layers.conv5_2',\n",
       " 'conv_layers.ReLU5_2',\n",
       " 'conv_layers.MaxPool5',\n",
       " 'fullyconnected_layers.fc6',\n",
       " 'fullyconnected_layers.ReLU6',\n",
       " 'fullyconnected_layers.fc7',\n",
       " 'fullyconnected_layers.ReLU7',\n",
       " 'fullyconnected_layers.fc8a',\n",
       " 'fullyconnected_layers.Softmax8a']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, eval_nodes = get_graph_node_names(vgg16_tweaked)\n",
    "eval_nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
