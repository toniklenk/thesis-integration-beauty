{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classes\n",
    "import caffemodel2pytorch as caffe\n",
    "\n",
    "# modified visualpriors library\n",
    "from transforms import VisualPriorRepresentation, VisualPriorPredictedLabel\n",
    "from taskonomy_network import TaskonomyEncoder, TaskonomyDecoder\n",
    "\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchsummary\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import torch.utils.model_zoo # required to load nets\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch import nn\n",
    "import torchvision.models\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Taskonomy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'autoencoding'\n",
    "\n",
    "VisualPriorRepresentation._load_unloaded_nets([task]) # preload nets\n",
    "net = VisualPriorRepresentation.feature_task_to_net[task] # loads encoder only for representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchsummary.summary(net, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tweaked net for retrieving activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes, eval_nodes = get_graph_node_names(net)\n",
    "print(len(eval_nodes))\n",
    "eval_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_nodes = { k:k for k in ['conv1']+[node for node in eval_nodes if \"conv3\" in node]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_tweaked = create_feature_extractor(net, return_nodes=return_nodes)\n",
    "net_tweaked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Image, convert to tensor, create image parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkerboard(scale, output_size=640):\n",
    "    board = np.indices((scale,scale)).sum(axis=0) % 2\n",
    "    board = board.repeat(output_size/scale,axis=0).repeat(output_size/scale, axis=1)\n",
    "    board = board[:,:, np.newaxis,].repeat(3,axis=2)\n",
    "    return board.astype(dtype = np.bool_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('./data/stimuli_places1/Places365_val_00036011.jpg')\n",
    "img = img.resize((640,640))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(img)\n",
    "\n",
    "img1, img2 = np.where(checkerboard(4), img, 255), np.where(~checkerboard(4), img, 255)\n",
    "\n",
    "img, img1, img2 = Image.fromarray(img), Image.fromarray(img1), Image.fromarray(img2)\n",
    "img, img1, img2 = TF.to_tensor(TF.resize(img, 256)) * 2 - 1, TF.to_tensor(TF.resize(img1, 256)) * 2 - 1, TF.to_tensor(TF.resize(img2, 256)) * 2 - 1\n",
    "img, img1, img2 = img.unsqueeze_(0), img1.unsqueeze_(0), img2.unsqueeze_(0)\n",
    "img.shape, img1.shape, img2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get activations for image and image parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_act = net_tweaked(img)\n",
    "img1_act = net_tweaked(img1)\n",
    "img2_act = net_tweaked(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate average activation for image parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img12avg_act = { k:None for k in img_act.keys()}\n",
    "img12avg_act.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in img_act.keys():\n",
    "    img12avg_act[layer] = torch.stack((img1_act[layer], img2_act[layer]), dim=0).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sanity check on average partial and full activations\")\n",
    "for layer in img_act.keys():\n",
    "    print(img_act[layer].shape == img12avg_act[layer].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration coeficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration = { k:None for k in img_act.keys()}\n",
    "\n",
    "for (layer_name, a1, a2) in zip(integration.keys(), img_act.values(), img12avg_act.values()):\n",
    "    integration[layer_name] = pearsonr(a1.flatten(), a2.flatten())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(integration.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read beauty ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('~/Documents/Thesis/Taskonomy Integration/behavior/ratings_study1.csv', header=None).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/home/max/Documents/Thesis/Taskonomy Integration/data/stimuli_places1'\n",
    "l = list(f for f in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, f)) and f.endswith('.jpg'))\n",
    "l.sort()\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_dataset= classes.ImageDataset('/home/max/Documents/Thesis/Taskonomy Integration/data/stimuli_places1')\n",
    "pl_it = iter(pl_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlate integration with beauty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = classes.IntegrationCalculator(net, return_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_dataset= classes.ImageDataset('./data/stimuli_places1', beauty_ratings_path='./behavior/ratings_study1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for idx, img in enumerate(iter(pl_dataset)):\n",
    "    l.append(inc.integration_coeff(img))\n",
    "    if idx == 40:\n",
    "        break\n",
    "\n",
    "res = pd.DataFrame(l, columns=inc.evalutation_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_dataset.beauty_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.aggregate(lambda x: pearsonr(x, ratings)[0], axis= 0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   X: Integration measures, DataFrame (num_images x num_layers)\n",
    "   y: Beauty reatings, Series (num_images x 1)\n",
    "\"\"\"\n",
    "pl_dataset.beauty_ratings\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(res)\n",
    "y = StandardScaler().fit_transform(pd.DataFrame(pl_dataset.beauty_ratings)[0:40+1])\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n",
    "g = loo.split(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, te = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[t];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X[t], y[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(X[te]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n",
    "l = []\n",
    "for train_idx, predict_idx in loo.split(X):\n",
    "    glm = LinearRegression().fit(X[train_idx], y[train_idx])\n",
    "    l.append(glm.predict(X[predict_idx]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.Series(l)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_dataset.beauty_ratings[0:40+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(predictions, pl_dataset.beauty_ratings[0:40+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking implementation with vgg16_places net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vgg16_places365 weights from converted caffe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caffemodel2pytorch: loading model from [vgg16_places365.caffemodel] in HDF5 format failed [Unable to open file (file signature not found)], falling back to caffemodel format\n",
      "caffemodel2pytorch: loaded model from [vgg16_places365.caffemodel] in caffemodel format\n"
     ]
    }
   ],
   "source": [
    "# can only do inference with this, but cannot retrieve intermediate activations\n",
    "vgg16_places_inference_net = caffe.Net(prototxt='deploy_vgg16_places365.prototxt', weights='vgg16_places365.caffemodel', caffe_proto='https://raw.githubusercontent.com/BVLC/caffe/master/src/caffe/proto/caffe.proto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information about model architecture nevertheless\n",
    "torchsummary.summary(vgg16_places_inference_net, input_size=(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load converted model to check for additional information\n",
    "vgg16_places_statedict = torch.load('vgg16_places365.caffemodel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parameter names (need to match those in model definition)\n",
    "list(name for name, _ in vgg16_places_statedict.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect a vanilla vgg16 network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vgg16_vanilla = torchvision.models.vgg16()\n",
    "#vgg16_vanilla.features = torch.nn.Sequential(collections.OrderedDict(zip(['conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', 'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2', 'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'pool3', 'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3', 'relu4_3', 'pool4', 'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3', 'pool5'], vgg16_vanilla.features)))\n",
    "#vgg16_vanilla.classifier = torch.nn.Sequential(collections.OrderedDict(zip(['fc6', 'relu6', 'drop6', 'fc7', 'relu7', 'drop7', 'fc8'], vgg16_vanilla.classifier)))\n",
    "torchsummary.summary(vgg16_vanilla, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise vgg16_places365 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_places = nn.Sequential(\n",
    "    collections.OrderedDict(\n",
    "        [\n",
    "            ('conv1_1', nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU1_1', nn.ReLU()),\n",
    "            ('conv1_2', nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU1_2',nn.ReLU()),\n",
    "            ('MaxPool1', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "            ('conv2_1',nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU2_1',nn.ReLU()),\n",
    "            ('conv2_2',nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU2_2',nn.ReLU()),\n",
    "            ('MaxPool2', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "            ('conv3_1', nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU3_1',nn.ReLU()),\n",
    "            ('conv3_2', nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU3_2',nn.ReLU()),\n",
    "            ('conv3_3', nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU3_3',nn.ReLU()),\n",
    "            ('MaxPool3', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "            ('conv4_1', nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU4_1',nn.ReLU()),\n",
    "            ('conv4_2', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU4_2',nn.ReLU()),\n",
    "            ('conv4_3', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU4_3',nn.ReLU()),\n",
    "            ('MaxPool4', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "            ('conv5_1', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU5_1',nn.ReLU()),\n",
    "            ('conv5_2', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU5_2',nn.ReLU()),\n",
    "            ('conv5_3', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU5_3',nn.ReLU()),\n",
    "            ('MaxPool5', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "            ('Flatten6', nn.Flatten()),\n",
    "            ('fc6', nn.Linear(in_features=25088, out_features=4096)),\n",
    "            ('ReLU6',nn.ReLU()),\n",
    "            ('fc7', nn.Linear(in_features=4096, out_features=4096)),\n",
    "            ('ReLU7',nn.ReLU()),\n",
    "            ('fc8a', nn.Linear(in_features=4096, out_features=365)),\n",
    "            ('Softmax8a', nn.Softmax(dim=-1))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "torchsummary.summary(vgg16_places, input_size=(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_places.load_state_dict(torch.load('vgg16_places365.caffemodel.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
