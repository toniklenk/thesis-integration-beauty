{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classes\n",
    "import caffemodel2pytorch as caffe\n",
    "\n",
    "# modified visualpriors library\n",
    "from transforms import VisualPriorRepresentation, VisualPriorPredictedLabel\n",
    "from taskonomy_network import TaskonomyEncoder, TaskonomyDecoder\n",
    "\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchsummary\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.io import savemat\n",
    "\n",
    "import torch.utils.model_zoo # required to load nets\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch import nn\n",
    "import torchvision.models\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for one taskonomy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 28, 28]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 28, 28]             128\n",
      "             ReLU-32           [-1, 64, 28, 28]               0\n",
      "           Conv2d-33          [-1, 256, 28, 28]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 28, 28]             512\n",
      "        MaxPool2d-35          [-1, 256, 28, 28]               0\n",
      "             ReLU-36          [-1, 256, 28, 28]               0\n",
      "       Bottleneck-37          [-1, 256, 28, 28]               0\n",
      "           Conv2d-38          [-1, 128, 28, 28]          32,768\n",
      "      BatchNorm2d-39          [-1, 128, 28, 28]             256\n",
      "             ReLU-40          [-1, 128, 28, 28]               0\n",
      "           Conv2d-41          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-42          [-1, 128, 28, 28]             256\n",
      "             ReLU-43          [-1, 128, 28, 28]               0\n",
      "           Conv2d-44          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-45          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-46          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-47          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-48          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-49          [-1, 512, 28, 28]               0\n",
      "           Conv2d-50          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-51          [-1, 128, 28, 28]             256\n",
      "             ReLU-52          [-1, 128, 28, 28]               0\n",
      "           Conv2d-53          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
      "             ReLU-55          [-1, 128, 28, 28]               0\n",
      "           Conv2d-56          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-57          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-58          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-59          [-1, 512, 28, 28]               0\n",
      "           Conv2d-60          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "             ReLU-62          [-1, 128, 28, 28]               0\n",
      "           Conv2d-63          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "             ReLU-65          [-1, 128, 28, 28]               0\n",
      "           Conv2d-66          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-67          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-68          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-69          [-1, 512, 28, 28]               0\n",
      "           Conv2d-70          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-71          [-1, 128, 28, 28]             256\n",
      "             ReLU-72          [-1, 128, 28, 28]               0\n",
      "           Conv2d-73          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-74          [-1, 128, 14, 14]             256\n",
      "             ReLU-75          [-1, 128, 14, 14]               0\n",
      "           Conv2d-76          [-1, 512, 14, 14]          65,536\n",
      "      BatchNorm2d-77          [-1, 512, 14, 14]           1,024\n",
      "        MaxPool2d-78          [-1, 512, 14, 14]               0\n",
      "             ReLU-79          [-1, 512, 14, 14]               0\n",
      "       Bottleneck-80          [-1, 512, 14, 14]               0\n",
      "           Conv2d-81          [-1, 256, 14, 14]         131,072\n",
      "      BatchNorm2d-82          [-1, 256, 14, 14]             512\n",
      "             ReLU-83          [-1, 256, 14, 14]               0\n",
      "           Conv2d-84          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-85          [-1, 256, 14, 14]             512\n",
      "             ReLU-86          [-1, 256, 14, 14]               0\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-89         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-90         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-91         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-92         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-93          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-94          [-1, 256, 14, 14]             512\n",
      "             ReLU-95          [-1, 256, 14, 14]               0\n",
      "           Conv2d-96          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-97          [-1, 256, 14, 14]             512\n",
      "             ReLU-98          [-1, 256, 14, 14]               0\n",
      "           Conv2d-99         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-100         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-101         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-102         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-103          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-104          [-1, 256, 14, 14]             512\n",
      "            ReLU-105          [-1, 256, 14, 14]               0\n",
      "          Conv2d-106          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-107          [-1, 256, 14, 14]             512\n",
      "            ReLU-108          [-1, 256, 14, 14]               0\n",
      "          Conv2d-109         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-110         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-111         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-112         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-113          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-114          [-1, 256, 14, 14]             512\n",
      "            ReLU-115          [-1, 256, 14, 14]               0\n",
      "          Conv2d-116          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-117          [-1, 256, 14, 14]             512\n",
      "            ReLU-118          [-1, 256, 14, 14]               0\n",
      "          Conv2d-119         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-120         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-121         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-122         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-123          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-124          [-1, 256, 14, 14]             512\n",
      "            ReLU-125          [-1, 256, 14, 14]               0\n",
      "          Conv2d-126          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-127          [-1, 256, 14, 14]             512\n",
      "            ReLU-128          [-1, 256, 14, 14]               0\n",
      "          Conv2d-129         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-130         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-131         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-132         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-133          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-134          [-1, 256, 14, 14]             512\n",
      "            ReLU-135          [-1, 256, 14, 14]               0\n",
      "          Conv2d-136          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-137          [-1, 256, 14, 14]             512\n",
      "            ReLU-138          [-1, 256, 14, 14]               0\n",
      "          Conv2d-139         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-140         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-141         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-142         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-143          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-144          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-145          [-1, 512, 14, 14]               0\n",
      "          Conv2d-146          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-147          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-148          [-1, 512, 14, 14]               0\n",
      "          Conv2d-149         [-1, 2048, 14, 14]       1,048,576\n",
      "     BatchNorm2d-150         [-1, 2048, 14, 14]           4,096\n",
      "          Conv2d-151         [-1, 2048, 14, 14]       2,097,152\n",
      "     BatchNorm2d-152         [-1, 2048, 14, 14]           4,096\n",
      "            ReLU-153         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-154         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-155          [-1, 512, 14, 14]       1,048,576\n",
      "     BatchNorm2d-156          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-157          [-1, 512, 14, 14]               0\n",
      "          Conv2d-158          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-159          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-160          [-1, 512, 14, 14]               0\n",
      "          Conv2d-161         [-1, 2048, 14, 14]       1,048,576\n",
      "     BatchNorm2d-162         [-1, 2048, 14, 14]           4,096\n",
      "            ReLU-163         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-164         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-165          [-1, 512, 14, 14]       1,048,576\n",
      "     BatchNorm2d-166          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-167          [-1, 512, 14, 14]               0\n",
      "          Conv2d-168          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-169          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-170          [-1, 512, 14, 14]               0\n",
      "          Conv2d-171         [-1, 2048, 14, 14]       1,048,576\n",
      "     BatchNorm2d-172         [-1, 2048, 14, 14]           4,096\n",
      "            ReLU-173         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-174         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-175            [-1, 8, 14, 14]         147,456\n",
      "     BatchNorm2d-176            [-1, 8, 14, 14]              16\n",
      "            ReLU-177            [-1, 8, 14, 14]               0\n",
      "       GroupNorm-178            [-1, 8, 14, 14]               0\n",
      "================================================================\n",
      "Total params: 23,655,504\n",
      "Trainable params: 0\n",
      "Non-trainable params: 23,655,504\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.58\n",
      "Params size (MB): 90.24\n",
      "Estimated Total Size (MB): 377.40\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task = 'autoencoding'\n",
    "\n",
    "VisualPriorRepresentation._load_unloaded_nets([task]) # preload nets\n",
    "net = VisualPriorRepresentation.feature_task_to_net[task] # loads encoder only for representations\n",
    "torchsummary.summary(net, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x   pad   conv1   bn1   relu   maxpool   layer1.0.conv1   layer1.0.bn1   layer1.0.relu   layer1.0.conv2   layer1.0.bn2   layer1.0.relu_1   layer1.0.conv3   layer1.0.bn3   layer1.0.downsample.0   layer1.0.downsample.1   layer1.0.add   layer1.0.relu_2   layer1.1.conv1   layer1.1.bn1   layer1.1.relu   layer1.1.conv2   layer1.1.bn2   layer1.1.relu_1   layer1.1.conv3   layer1.1.bn3   layer1.1.add   layer1.1.relu_2   layer1.2.conv1   layer1.2.bn1   layer1.2.relu   layer1.2.conv2   layer1.2.bn2   layer1.2.relu_1   layer1.2.conv3   layer1.2.bn3   layer1.2.downsample.0   layer1.2.add   layer1.2.relu_2   layer2.0.conv1   layer2.0.bn1   layer2.0.relu   layer2.0.conv2   layer2.0.bn2   layer2.0.relu_1   layer2.0.conv3   layer2.0.bn3   layer2.0.downsample.0   layer2.0.downsample.1   layer2.0.add   layer2.0.relu_2   layer2.1.conv1   layer2.1.bn1   layer2.1.relu   layer2.1.conv2   layer2.1.bn2   layer2.1.relu_1   layer2.1.conv3   layer2.1.bn3   layer2.1.add   layer2.1.relu_2   layer2.2.conv1   layer2.2.bn1   layer2.2.relu   layer2.2.conv2   layer2.2.bn2   layer2.2.relu_1   layer2.2.conv3   layer2.2.bn3   layer2.2.add   layer2.2.relu_2   layer2.3.conv1   layer2.3.bn1   layer2.3.relu   layer2.3.conv2   layer2.3.bn2   layer2.3.relu_1   layer2.3.conv3   layer2.3.bn3   layer2.3.downsample.0   layer2.3.add   layer2.3.relu_2   layer3.0.conv1   layer3.0.bn1   layer3.0.relu   layer3.0.conv2   layer3.0.bn2   layer3.0.relu_1   layer3.0.conv3   layer3.0.bn3   layer3.0.downsample.0   layer3.0.downsample.1   layer3.0.add   layer3.0.relu_2   layer3.1.conv1   layer3.1.bn1   layer3.1.relu   layer3.1.conv2   layer3.1.bn2   layer3.1.relu_1   layer3.1.conv3   layer3.1.bn3   layer3.1.add   layer3.1.relu_2   layer3.2.conv1   layer3.2.bn1   layer3.2.relu   layer3.2.conv2   layer3.2.bn2   layer3.2.relu_1   layer3.2.conv3   layer3.2.bn3   layer3.2.add   layer3.2.relu_2   layer3.3.conv1   layer3.3.bn1   layer3.3.relu   layer3.3.conv2   layer3.3.bn2   layer3.3.relu_1   layer3.3.conv3   layer3.3.bn3   layer3.3.add   layer3.3.relu_2   layer3.4.conv1   layer3.4.bn1   layer3.4.relu   layer3.4.conv2   layer3.4.bn2   layer3.4.relu_1   layer3.4.conv3   layer3.4.bn3   layer3.4.add   layer3.4.relu_2   layer3.5.conv1   layer3.5.bn1   layer3.5.relu   layer3.5.conv2   layer3.5.bn2   layer3.5.relu_1   layer3.5.conv3   layer3.5.bn3   layer3.5.add   layer3.5.relu_2   layer4.0.conv1   layer4.0.bn1   layer4.0.relu   layer4.0.conv2   layer4.0.bn2   layer4.0.relu_1   layer4.0.conv3   layer4.0.bn3   layer4.0.downsample.0   layer4.0.downsample.1   layer4.0.add   layer4.0.relu_2   layer4.1.conv1   layer4.1.bn1   layer4.1.relu   layer4.1.conv2   layer4.1.bn2   layer4.1.relu_1   layer4.1.conv3   layer4.1.bn3   layer4.1.add   layer4.1.relu_2   layer4.2.conv1   layer4.2.bn1   layer4.2.relu   layer4.2.conv2   layer4.2.bn2   layer4.2.relu_1   layer4.2.conv3   layer4.2.bn3   layer4.2.add   layer4.2.relu_2   compress1   compress_bn   relu1   groupnorm   "
     ]
    }
   ],
   "source": [
    "_, eval_nodes = get_graph_node_names(net)\n",
    "for node in eval_nodes:\n",
    "    print(node, end='   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1': 'conv1',\n",
       " 'layer1.0.conv1': 'layer1.0.conv1',\n",
       " 'layer1.0.conv2': 'layer1.0.conv2',\n",
       " 'layer1.0.conv3': 'layer1.0.conv3',\n",
       " 'layer1.1.conv1': 'layer1.1.conv1',\n",
       " 'layer1.1.conv2': 'layer1.1.conv2',\n",
       " 'layer1.1.conv3': 'layer1.1.conv3',\n",
       " 'layer1.2.conv1': 'layer1.2.conv1',\n",
       " 'layer1.2.conv2': 'layer1.2.conv2',\n",
       " 'layer1.2.conv3': 'layer1.2.conv3',\n",
       " 'layer2.0.conv1': 'layer2.0.conv1',\n",
       " 'layer2.0.conv2': 'layer2.0.conv2',\n",
       " 'layer2.0.conv3': 'layer2.0.conv3',\n",
       " 'layer2.1.conv1': 'layer2.1.conv1',\n",
       " 'layer2.1.conv2': 'layer2.1.conv2',\n",
       " 'layer2.1.conv3': 'layer2.1.conv3',\n",
       " 'layer2.2.conv1': 'layer2.2.conv1',\n",
       " 'layer2.2.conv2': 'layer2.2.conv2',\n",
       " 'layer2.2.conv3': 'layer2.2.conv3',\n",
       " 'layer2.3.conv1': 'layer2.3.conv1',\n",
       " 'layer2.3.conv2': 'layer2.3.conv2',\n",
       " 'layer2.3.conv3': 'layer2.3.conv3',\n",
       " 'layer3.0.conv1': 'layer3.0.conv1',\n",
       " 'layer3.0.conv2': 'layer3.0.conv2',\n",
       " 'layer3.0.conv3': 'layer3.0.conv3',\n",
       " 'layer3.1.conv1': 'layer3.1.conv1',\n",
       " 'layer3.1.conv2': 'layer3.1.conv2',\n",
       " 'layer3.1.conv3': 'layer3.1.conv3',\n",
       " 'layer3.2.conv1': 'layer3.2.conv1',\n",
       " 'layer3.2.conv2': 'layer3.2.conv2',\n",
       " 'layer3.2.conv3': 'layer3.2.conv3',\n",
       " 'layer3.3.conv1': 'layer3.3.conv1',\n",
       " 'layer3.3.conv2': 'layer3.3.conv2',\n",
       " 'layer3.3.conv3': 'layer3.3.conv3',\n",
       " 'layer3.4.conv1': 'layer3.4.conv1',\n",
       " 'layer3.4.conv2': 'layer3.4.conv2',\n",
       " 'layer3.4.conv3': 'layer3.4.conv3',\n",
       " 'layer3.5.conv1': 'layer3.5.conv1',\n",
       " 'layer3.5.conv2': 'layer3.5.conv2',\n",
       " 'layer3.5.conv3': 'layer3.5.conv3',\n",
       " 'layer4.0.conv1': 'layer4.0.conv1',\n",
       " 'layer4.0.conv2': 'layer4.0.conv2',\n",
       " 'layer4.0.conv3': 'layer4.0.conv3',\n",
       " 'layer4.1.conv1': 'layer4.1.conv1',\n",
       " 'layer4.1.conv2': 'layer4.1.conv2',\n",
       " 'layer4.1.conv3': 'layer4.1.conv3',\n",
       " 'layer4.2.conv1': 'layer4.2.conv1',\n",
       " 'layer4.2.conv2': 'layer4.2.conv2',\n",
       " 'layer4.2.conv3': 'layer4.2.conv3'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_nodes = { node:node for node in eval_nodes if \"conv\" in node or 'fc' in node}\n",
    "return_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskonomyEncoder(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (layer1): Module(\n",
       "    (0): Module(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Module(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Module(\n",
       "        (0): MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Module(\n",
       "    (0): Module(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Module(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Module(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Module(\n",
       "        (0): MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Module(\n",
       "    (0): Module(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Module(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Module(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): Module(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): Module(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Module(\n",
       "    (0): Module(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Module(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_tweaked = create_feature_extractor(net, return_nodes=return_nodes)\n",
    "net_tweaked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_images = classes.ImageDataset('./data/stimuli_places1', beauty_ratings_path='./behavior/ratings_study1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_calc = classes.IntegrationCalculator(net_tweaked, return_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [14:17,  3.43s/it]\n"
     ]
    }
   ],
   "source": [
    "result = classes.calculate_dataset_integration(places_images, integration_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv1</th>\n",
       "      <th>layer1.0.conv1</th>\n",
       "      <th>layer1.0.conv2</th>\n",
       "      <th>layer1.0.conv3</th>\n",
       "      <th>layer1.1.conv1</th>\n",
       "      <th>layer1.1.conv2</th>\n",
       "      <th>layer1.1.conv3</th>\n",
       "      <th>layer1.2.conv1</th>\n",
       "      <th>layer1.2.conv2</th>\n",
       "      <th>layer1.2.conv3</th>\n",
       "      <th>...</th>\n",
       "      <th>layer3.5.conv3</th>\n",
       "      <th>layer4.0.conv1</th>\n",
       "      <th>layer4.0.conv2</th>\n",
       "      <th>layer4.0.conv3</th>\n",
       "      <th>layer4.1.conv1</th>\n",
       "      <th>layer4.1.conv2</th>\n",
       "      <th>layer4.1.conv3</th>\n",
       "      <th>layer4.2.conv1</th>\n",
       "      <th>layer4.2.conv2</th>\n",
       "      <th>layer4.2.conv3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.901755</td>\n",
       "      <td>-0.927244</td>\n",
       "      <td>-0.906128</td>\n",
       "      <td>-0.892599</td>\n",
       "      <td>-0.924830</td>\n",
       "      <td>-0.857169</td>\n",
       "      <td>-0.839657</td>\n",
       "      <td>-0.941186</td>\n",
       "      <td>-0.896023</td>\n",
       "      <td>-0.852555</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.859965</td>\n",
       "      <td>-0.995464</td>\n",
       "      <td>-0.858975</td>\n",
       "      <td>-0.725659</td>\n",
       "      <td>-0.895166</td>\n",
       "      <td>-0.546067</td>\n",
       "      <td>-0.366636</td>\n",
       "      <td>-0.620608</td>\n",
       "      <td>-0.348664</td>\n",
       "      <td>-0.371284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.977258</td>\n",
       "      <td>-0.979798</td>\n",
       "      <td>-0.977366</td>\n",
       "      <td>-0.970708</td>\n",
       "      <td>-0.973443</td>\n",
       "      <td>-0.941702</td>\n",
       "      <td>-0.918240</td>\n",
       "      <td>-0.974717</td>\n",
       "      <td>-0.968564</td>\n",
       "      <td>-0.947710</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.938060</td>\n",
       "      <td>-0.996411</td>\n",
       "      <td>-0.912282</td>\n",
       "      <td>-0.805249</td>\n",
       "      <td>-0.957089</td>\n",
       "      <td>-0.706170</td>\n",
       "      <td>-0.470574</td>\n",
       "      <td>-0.755524</td>\n",
       "      <td>-0.356730</td>\n",
       "      <td>-0.136613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.985005</td>\n",
       "      <td>-0.988065</td>\n",
       "      <td>-0.982306</td>\n",
       "      <td>-0.978973</td>\n",
       "      <td>-0.980807</td>\n",
       "      <td>-0.959042</td>\n",
       "      <td>-0.937909</td>\n",
       "      <td>-0.980815</td>\n",
       "      <td>-0.977924</td>\n",
       "      <td>-0.965159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.932084</td>\n",
       "      <td>-0.996749</td>\n",
       "      <td>-0.916451</td>\n",
       "      <td>-0.840916</td>\n",
       "      <td>-0.964348</td>\n",
       "      <td>-0.723489</td>\n",
       "      <td>-0.497462</td>\n",
       "      <td>-0.776550</td>\n",
       "      <td>-0.360851</td>\n",
       "      <td>-0.148527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.969972</td>\n",
       "      <td>-0.971745</td>\n",
       "      <td>-0.968256</td>\n",
       "      <td>-0.959294</td>\n",
       "      <td>-0.971802</td>\n",
       "      <td>-0.943989</td>\n",
       "      <td>-0.924279</td>\n",
       "      <td>-0.974445</td>\n",
       "      <td>-0.967046</td>\n",
       "      <td>-0.948057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.903482</td>\n",
       "      <td>-0.997028</td>\n",
       "      <td>-0.915133</td>\n",
       "      <td>-0.862330</td>\n",
       "      <td>-0.956345</td>\n",
       "      <td>-0.715045</td>\n",
       "      <td>-0.582552</td>\n",
       "      <td>-0.792562</td>\n",
       "      <td>-0.507871</td>\n",
       "      <td>-0.420497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.982985</td>\n",
       "      <td>-0.985039</td>\n",
       "      <td>-0.980419</td>\n",
       "      <td>-0.975774</td>\n",
       "      <td>-0.981731</td>\n",
       "      <td>-0.966215</td>\n",
       "      <td>-0.950199</td>\n",
       "      <td>-0.980384</td>\n",
       "      <td>-0.976013</td>\n",
       "      <td>-0.963792</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.902900</td>\n",
       "      <td>-0.996946</td>\n",
       "      <td>-0.916627</td>\n",
       "      <td>-0.884209</td>\n",
       "      <td>-0.963456</td>\n",
       "      <td>-0.755956</td>\n",
       "      <td>-0.626542</td>\n",
       "      <td>-0.805912</td>\n",
       "      <td>-0.571683</td>\n",
       "      <td>-0.499548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>-0.967946</td>\n",
       "      <td>-0.978558</td>\n",
       "      <td>-0.971832</td>\n",
       "      <td>-0.965229</td>\n",
       "      <td>-0.966798</td>\n",
       "      <td>-0.933618</td>\n",
       "      <td>-0.914758</td>\n",
       "      <td>-0.967229</td>\n",
       "      <td>-0.955363</td>\n",
       "      <td>-0.933746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926590</td>\n",
       "      <td>-0.995430</td>\n",
       "      <td>-0.902838</td>\n",
       "      <td>-0.806128</td>\n",
       "      <td>-0.947222</td>\n",
       "      <td>-0.727485</td>\n",
       "      <td>-0.504238</td>\n",
       "      <td>-0.768200</td>\n",
       "      <td>-0.392083</td>\n",
       "      <td>-0.184156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>-0.971098</td>\n",
       "      <td>-0.974567</td>\n",
       "      <td>-0.971881</td>\n",
       "      <td>-0.963860</td>\n",
       "      <td>-0.971524</td>\n",
       "      <td>-0.946957</td>\n",
       "      <td>-0.928143</td>\n",
       "      <td>-0.972739</td>\n",
       "      <td>-0.964131</td>\n",
       "      <td>-0.943081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.925275</td>\n",
       "      <td>-0.996563</td>\n",
       "      <td>-0.914013</td>\n",
       "      <td>-0.855937</td>\n",
       "      <td>-0.954991</td>\n",
       "      <td>-0.736623</td>\n",
       "      <td>-0.608247</td>\n",
       "      <td>-0.798507</td>\n",
       "      <td>-0.543984</td>\n",
       "      <td>-0.475614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>-0.970213</td>\n",
       "      <td>-0.975484</td>\n",
       "      <td>-0.968955</td>\n",
       "      <td>-0.964136</td>\n",
       "      <td>-0.970212</td>\n",
       "      <td>-0.949807</td>\n",
       "      <td>-0.937206</td>\n",
       "      <td>-0.966894</td>\n",
       "      <td>-0.957912</td>\n",
       "      <td>-0.952441</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.924776</td>\n",
       "      <td>-0.993317</td>\n",
       "      <td>-0.912605</td>\n",
       "      <td>-0.901262</td>\n",
       "      <td>-0.942384</td>\n",
       "      <td>-0.802596</td>\n",
       "      <td>-0.611354</td>\n",
       "      <td>-0.816946</td>\n",
       "      <td>-0.436379</td>\n",
       "      <td>-0.208226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>-0.991683</td>\n",
       "      <td>-0.993783</td>\n",
       "      <td>-0.991791</td>\n",
       "      <td>-0.989363</td>\n",
       "      <td>-0.987538</td>\n",
       "      <td>-0.974684</td>\n",
       "      <td>-0.956181</td>\n",
       "      <td>-0.986093</td>\n",
       "      <td>-0.987357</td>\n",
       "      <td>-0.976262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.924651</td>\n",
       "      <td>-0.997633</td>\n",
       "      <td>-0.923652</td>\n",
       "      <td>-0.856157</td>\n",
       "      <td>-0.972104</td>\n",
       "      <td>-0.729208</td>\n",
       "      <td>-0.493144</td>\n",
       "      <td>-0.781738</td>\n",
       "      <td>-0.362910</td>\n",
       "      <td>-0.120459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>-0.986087</td>\n",
       "      <td>-0.985298</td>\n",
       "      <td>-0.984929</td>\n",
       "      <td>-0.979691</td>\n",
       "      <td>-0.986239</td>\n",
       "      <td>-0.975361</td>\n",
       "      <td>-0.969088</td>\n",
       "      <td>-0.987772</td>\n",
       "      <td>-0.984247</td>\n",
       "      <td>-0.973219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.894974</td>\n",
       "      <td>-0.997936</td>\n",
       "      <td>-0.921355</td>\n",
       "      <td>-0.899193</td>\n",
       "      <td>-0.969977</td>\n",
       "      <td>-0.780987</td>\n",
       "      <td>-0.716732</td>\n",
       "      <td>-0.824350</td>\n",
       "      <td>-0.700911</td>\n",
       "      <td>-0.661872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        conv1  layer1.0.conv1  layer1.0.conv2  layer1.0.conv3  layer1.1.conv1  \\\n",
       "0   -0.901755       -0.927244       -0.906128       -0.892599       -0.924830   \n",
       "1   -0.977258       -0.979798       -0.977366       -0.970708       -0.973443   \n",
       "2   -0.985005       -0.988065       -0.982306       -0.978973       -0.980807   \n",
       "3   -0.969972       -0.971745       -0.968256       -0.959294       -0.971802   \n",
       "4   -0.982985       -0.985039       -0.980419       -0.975774       -0.981731   \n",
       "..        ...             ...             ...             ...             ...   \n",
       "245 -0.967946       -0.978558       -0.971832       -0.965229       -0.966798   \n",
       "246 -0.971098       -0.974567       -0.971881       -0.963860       -0.971524   \n",
       "247 -0.970213       -0.975484       -0.968955       -0.964136       -0.970212   \n",
       "248 -0.991683       -0.993783       -0.991791       -0.989363       -0.987538   \n",
       "249 -0.986087       -0.985298       -0.984929       -0.979691       -0.986239   \n",
       "\n",
       "     layer1.1.conv2  layer1.1.conv3  layer1.2.conv1  layer1.2.conv2  \\\n",
       "0         -0.857169       -0.839657       -0.941186       -0.896023   \n",
       "1         -0.941702       -0.918240       -0.974717       -0.968564   \n",
       "2         -0.959042       -0.937909       -0.980815       -0.977924   \n",
       "3         -0.943989       -0.924279       -0.974445       -0.967046   \n",
       "4         -0.966215       -0.950199       -0.980384       -0.976013   \n",
       "..              ...             ...             ...             ...   \n",
       "245       -0.933618       -0.914758       -0.967229       -0.955363   \n",
       "246       -0.946957       -0.928143       -0.972739       -0.964131   \n",
       "247       -0.949807       -0.937206       -0.966894       -0.957912   \n",
       "248       -0.974684       -0.956181       -0.986093       -0.987357   \n",
       "249       -0.975361       -0.969088       -0.987772       -0.984247   \n",
       "\n",
       "     layer1.2.conv3  ...  layer3.5.conv3  layer4.0.conv1  layer4.0.conv2  \\\n",
       "0         -0.852555  ...       -0.859965       -0.995464       -0.858975   \n",
       "1         -0.947710  ...       -0.938060       -0.996411       -0.912282   \n",
       "2         -0.965159  ...       -0.932084       -0.996749       -0.916451   \n",
       "3         -0.948057  ...       -0.903482       -0.997028       -0.915133   \n",
       "4         -0.963792  ...       -0.902900       -0.996946       -0.916627   \n",
       "..              ...  ...             ...             ...             ...   \n",
       "245       -0.933746  ...       -0.926590       -0.995430       -0.902838   \n",
       "246       -0.943081  ...       -0.925275       -0.996563       -0.914013   \n",
       "247       -0.952441  ...       -0.924776       -0.993317       -0.912605   \n",
       "248       -0.976262  ...       -0.924651       -0.997633       -0.923652   \n",
       "249       -0.973219  ...       -0.894974       -0.997936       -0.921355   \n",
       "\n",
       "     layer4.0.conv3  layer4.1.conv1  layer4.1.conv2  layer4.1.conv3  \\\n",
       "0         -0.725659       -0.895166       -0.546067       -0.366636   \n",
       "1         -0.805249       -0.957089       -0.706170       -0.470574   \n",
       "2         -0.840916       -0.964348       -0.723489       -0.497462   \n",
       "3         -0.862330       -0.956345       -0.715045       -0.582552   \n",
       "4         -0.884209       -0.963456       -0.755956       -0.626542   \n",
       "..              ...             ...             ...             ...   \n",
       "245       -0.806128       -0.947222       -0.727485       -0.504238   \n",
       "246       -0.855937       -0.954991       -0.736623       -0.608247   \n",
       "247       -0.901262       -0.942384       -0.802596       -0.611354   \n",
       "248       -0.856157       -0.972104       -0.729208       -0.493144   \n",
       "249       -0.899193       -0.969977       -0.780987       -0.716732   \n",
       "\n",
       "     layer4.2.conv1  layer4.2.conv2  layer4.2.conv3  \n",
       "0         -0.620608       -0.348664       -0.371284  \n",
       "1         -0.755524       -0.356730       -0.136613  \n",
       "2         -0.776550       -0.360851       -0.148527  \n",
       "3         -0.792562       -0.507871       -0.420497  \n",
       "4         -0.805912       -0.571683       -0.499548  \n",
       "..              ...             ...             ...  \n",
       "245       -0.768200       -0.392083       -0.184156  \n",
       "246       -0.798507       -0.543984       -0.475614  \n",
       "247       -0.816946       -0.436379       -0.208226  \n",
       "248       -0.781738       -0.362910       -0.120459  \n",
       "249       -0.824350       -0.700911       -0.661872  \n",
       "\n",
       "[250 rows x 49 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for all taskonomy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_taskonomy_encoder_integration_analysis(task, places_images):\n",
    "    VisualPriorRepresentation._load_unloaded_nets([task])\n",
    "    net = VisualPriorRepresentation.feature_task_to_net[task]\n",
    "\n",
    "    _, eval_nodes = get_graph_node_names(net)\n",
    "    return_nodes = { node:node for node in eval_nodes if \"conv\" in node or 'fc' in node}\n",
    "    net_tweaked = create_feature_extractor(net, return_nodes=return_nodes)\n",
    "\n",
    "    integration_calc = classes.IntegrationCalculator(net_tweaked, return_nodes)\n",
    "    return classes.calculate_dataset_integration(places_images, integration_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ('autoencoding','depth_euclidean','jigsaw','reshading',\n",
    "         'colorization', 'edge_occlusion','keypoints2d','room_layout',\n",
    "         'curvature','edge_texture','keypoints3d','segment_unsup2d',\n",
    "         'class_object','egomotion','nonfixated_pose','segment_unsup25d',\n",
    "         'class_scene','fixated_pose','normal','segment_semantic',\n",
    "         'denoising','inpainting','point_matching','vanishing_point')\n",
    "\n",
    "places_images = classes.ImageDataset('./data/stimuli_places1', beauty_ratings_path='./behavior/ratings_study1.csv')\n",
    "\n",
    "mdict = {}\n",
    "for task in tasks:\n",
    "    task_result = run_taskonomy_encoder_integration_analysis(task, places_images)\n",
    "    task_result.to_csv(os.path.join('results', task, task + '.csv'))\n",
    "    #mdict[task] = run_taskonomy_encoder_integration_analysis(task, places_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#savemat(\"taskonomy_integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250, 49), (250, 1))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = StandardScaler().fit_transform(result)\n",
    "y = StandardScaler().fit_transform(pd.DataFrame(places_images.beauty_ratings))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo, predictions = LeaveOneOut(), []\n",
    "\n",
    "for train_idx, predict_idx in loo.split(X):\n",
    "    glm = LinearRegression().fit(X[train_idx], y[train_idx])\n",
    "    predictions.append(glm.predict(X[predict_idx]).item())\n",
    "\n",
    "predictions = pd.Series(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1749906402784313, 0.005530053362831548)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(predictions, places_images.beauty_ratings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
