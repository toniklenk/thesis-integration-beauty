{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classes\n",
    "import caffemodel2pytorch as caffe\n",
    "\n",
    "# modified visualpriors library\n",
    "from transforms import VisualPriorRepresentation, VisualPriorPredictedLabel\n",
    "from taskonomy_network import TaskonomyEncoder, TaskonomyDecoder\n",
    "\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchsummary\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.io import savemat\n",
    "\n",
    "import torch.utils.model_zoo # required to load nets\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch import nn\n",
    "import torchvision.models\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run analysis for vgg16_places\n",
    "\n",
    "To confirm correcteness of implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg16_places_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vgg16_places_model, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            collections.OrderedDict(\n",
    "                [\n",
    "                    ('conv1_1', nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU1_1', nn.ReLU()),\n",
    "                    ('conv1_2', nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU1_2',nn.ReLU()),\n",
    "                    ('MaxPool1', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "                    ('conv2_1',nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU2_1',nn.ReLU()),\n",
    "                    ('conv2_2',nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU2_2',nn.ReLU()),\n",
    "                    ('MaxPool2', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "                    ('conv3_1', nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU3_1',nn.ReLU()),\n",
    "                    ('conv3_2', nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU3_2',nn.ReLU()),\n",
    "                    ('conv3_3', nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU3_3',nn.ReLU()),\n",
    "                    ('MaxPool3', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "                    ('conv4_1', nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU4_1',nn.ReLU()),\n",
    "                    ('conv4_2', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU4_2',nn.ReLU()),\n",
    "                    ('conv4_3', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU4_3',nn.ReLU()),\n",
    "                    ('MaxPool4', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "                    ('conv5_1', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU5_1',nn.ReLU()),\n",
    "                    ('conv5_2', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU5_2',nn.ReLU()),\n",
    "                    ('conv5_3', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "                    ('ReLU5_3',nn.ReLU()),\n",
    "                    ('MaxPool5', nn.MaxPool2d(kernel_size=2, stride=2,padding=0))\n",
    "            ]))\n",
    "        self.fullyconnected_layers = nn.Sequential(\n",
    "            collections.OrderedDict(\n",
    "                [\n",
    "                    ('fc6', nn.Linear(in_features=25088, out_features=4096)),\n",
    "                    ('ReLU6',nn.ReLU()),\n",
    "                    ('fc7', nn.Linear(in_features=4096, out_features=4096)),\n",
    "                    ('ReLU7',nn.ReLU()),\n",
    "                    ('fc8a', nn.Linear(in_features=4096, out_features=365)),\n",
    "                    ('Softmax8a', nn.Softmax(dim=-1))\n",
    "            ]))\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fullyconnected_layers(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vgg16_places_model(\n",
       "  (conv_layers): Sequential(\n",
       "    (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (ReLU1_1): ReLU()\n",
       "    (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (ReLU1_2): ReLU()\n",
       "    (MaxPool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (ReLU2_1): ReLU()\n",
       "    (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (ReLU2_2): ReLU()\n",
       "    (MaxPool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (ReLU3_1): ReLU()\n",
       "    (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (ReLU3_2): ReLU()\n",
       "    (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (ReLU3_3): ReLU()\n",
       "    (MaxPool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (ReLU4_1): ReLU()\n",
       "    (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (ReLU4_2): ReLU()\n",
       "    (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (ReLU4_3): ReLU()\n",
       "    (MaxPool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (ReLU5_1): ReLU()\n",
       "    (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (ReLU5_2): ReLU()\n",
       "    (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (ReLU5_3): ReLU()\n",
       "    (MaxPool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fullyconnected_layers): Sequential(\n",
       "    (fc6): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (ReLU6): ReLU()\n",
       "    (fc7): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (ReLU7): ReLU()\n",
       "    (fc8a): Linear(in_features=4096, out_features=365, bias=True)\n",
       "    (Softmax8a): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = vgg16_places_model()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x   conv_layers.conv1   conv_layers.ReLU1   conv_layers.conv1_1   conv_layers.ReLU1_1   conv_layers.MaxPool1   conv_layers.conv2   conv_layers.ReLU2   conv_layers.conv2_1   conv_layers.ReLU2_1   conv_layers.MaxPool2   conv_layers.conv3   conv_layers.ReLU3   conv_layers.conv3_1   conv_layers.ReLU3_1   conv_layers.conv3_2   conv_layers.ReLU3_2   conv_layers.MaxPool3   conv_layers.conv4   conv_layers.ReLU4   conv_layers.conv4_1   conv_layers.ReLU4_1   conv_layers.conv4_2   conv_layers.ReLU4_2   conv_layers.MaxPool4   conv_layers.conv5   conv_layers.ReLU5   conv_layers.conv5_1   conv_layers.ReLU5_1   conv_layers.conv5_2   conv_layers.ReLU5_2   conv_layers.MaxPool5   fullyconnected_layers.fc6   fullyconnected_layers.ReLU6   fullyconnected_layers.fc7   fullyconnected_layers.ReLU7   fullyconnected_layers.fc8a   fullyconnected_layers.Softmax8a   "
     ]
    }
   ],
   "source": [
    "_, eval_nodes = get_graph_node_names(test)\n",
    "for node in eval_nodes:\n",
    "    print(node, end='   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "          Flatten-32                [-1, 25088]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "           Linear-35                 [-1, 4096]      16,781,312\n",
      "             ReLU-36                 [-1, 4096]               0\n",
      "           Linear-37                  [-1, 365]       1,495,405\n",
      "          Softmax-38                  [-1, 365]               0\n",
      "================================================================\n",
      "Total params: 135,755,949\n",
      "Trainable params: 135,755,949\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.72\n",
      "Params size (MB): 517.87\n",
      "Estimated Total Size (MB): 737.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vgg16_places = nn.Sequential(\n",
    "    collections.OrderedDict(\n",
    "        [\n",
    "            ('conv1_1', nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU1_1', nn.ReLU()),\n",
    "            ('conv1_2', nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU1_2',nn.ReLU()),\n",
    "            ('MaxPool1', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "            ('conv2_1',nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU2_1',nn.ReLU()),\n",
    "            ('conv2_2',nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU2_2',nn.ReLU()),\n",
    "            ('MaxPool2', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "            ('conv3_1', nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU3_1',nn.ReLU()),\n",
    "            ('conv3_2', nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU3_2',nn.ReLU()),\n",
    "            ('conv3_3', nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU3_3',nn.ReLU()),\n",
    "            ('MaxPool3', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "            ('conv4_1', nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU4_1',nn.ReLU()),\n",
    "            ('conv4_2', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU4_2',nn.ReLU()),\n",
    "            ('conv4_3', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU4_3',nn.ReLU()),\n",
    "            ('MaxPool4', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "            ('conv5_1', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU5_1',nn.ReLU()),\n",
    "            ('conv5_2', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU5_2',nn.ReLU()),\n",
    "            ('conv5_3', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ReLU5_3',nn.ReLU()),\n",
    "            ('MaxPool5', nn.MaxPool2d(kernel_size=2, stride=2,padding=0)),\n",
    "            #('Flatten6', nn.Flatten()),\n",
    "            ('fc6', nn.Linear(in_features=25088, out_features=4096)),\n",
    "            ('ReLU6',nn.ReLU()),\n",
    "            ('fc7', nn.Linear(in_features=4096, out_features=4096)),\n",
    "            ('ReLU7',nn.ReLU()),\n",
    "            ('fc8a', nn.Linear(in_features=4096, out_features=365)),\n",
    "            ('Softmax8a', nn.Softmax(dim=-1))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "torchsummary.summary(vgg16_places, input_size=(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU1_1): ReLU()\n",
       "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU1_2): ReLU()\n",
       "  (MaxPool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU2_1): ReLU()\n",
       "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU2_2): ReLU()\n",
       "  (MaxPool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU3_1): ReLU()\n",
       "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU3_2): ReLU()\n",
       "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU3_3): ReLU()\n",
       "  (MaxPool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU4_1): ReLU()\n",
       "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU4_2): ReLU()\n",
       "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU4_3): ReLU()\n",
       "  (MaxPool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU5_1): ReLU()\n",
       "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU5_2): ReLU()\n",
       "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ReLU5_3): ReLU()\n",
       "  (MaxPool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Flatten6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc6): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (ReLU6): ReLU()\n",
       "  (fc7): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (ReLU7): ReLU()\n",
       "  (fc8a): Linear(in_features=4096, out_features=365, bias=True)\n",
       "  (Softmax8a): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input   conv1   ReLU1   conv1_1   ReLU1_1   MaxPool1   conv2   ReLU2   conv2_1   ReLU2_1   MaxPool2   conv3   ReLU3   conv3_1   ReLU3_1   conv3_2   ReLU3_2   MaxPool3   conv4   ReLU4   conv4_1   ReLU4_1   conv4_2   ReLU4_2   MaxPool4   conv5   ReLU5   conv5_1   ReLU5_1   conv5_2   ReLU5_2   MaxPool5   Flatten6   fc6   ReLU6   fc7   ReLU7   fc8a   Softmax8a   "
     ]
    }
   ],
   "source": [
    "_, eval_nodes = get_graph_node_names(vgg16_places)\n",
    "for node in eval_nodes:\n",
    "    print(node, end='   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_nodes = { node:node for node in eval_nodes if \"conv\" in node or 'fc' in node}\n",
    "return_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe implement automatic extraction of all conv and fc layers, given just the style in specific network after inspection\n",
    "vgg16_places_fe = create_feature_extractor(vgg16_places, return_nodes=return_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_images = classes.ImageDataset('./data/stimuli_places1', beauty_ratings_path='./behavior/ratings_study1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_calc = classes.IntegrationCalculator(vgg16_places_fe, return_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x204800 and 25088x4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclasses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_dataset_integration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplaces_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintegration_calc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Thesis/Taskonomy Integration/classes.py:119\u001b[0m, in \u001b[0;36mcalculate_dataset_integration\u001b[0;34m(dataset_iterator, integration_calculator)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_dataset_integration\u001b[39m(dataset_iterator, integration_calculator: IntegrationCalculator):\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculate integration for whole dataset\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m        Input:\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m            dataset_iterator: class ImageDataset iterator that optionally specifies a image transformation\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m            integration: Integration calculator object that specifies DNN, layer to use and integration calculation procedure\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m        \u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m        Output:\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m            DataFrame: images x layers\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     lst \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m dataset_iterator:\n",
      "File \u001b[0;32m~/Documents/Thesis/Taskonomy Integration/classes.py:86\u001b[0m, in \u001b[0;36mIntegrationCalculator.integration_coeff\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     82\u001b[0m img, img1, img2 \u001b[38;5;241m=\u001b[39m TF\u001b[38;5;241m.\u001b[39mto_tensor(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), TF\u001b[38;5;241m.\u001b[39mto_tensor(img1)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), TF\u001b[38;5;241m.\u001b[39mto_tensor(img2)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# activation for full image\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m img_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# average activation of image parts\u001b[39;00m\n\u001b[1;32m     89\u001b[0m img1_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(img1)\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/fx/graph_module.py:738\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/fx/graph_module.py:317\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# noqa: TRY200\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/fx/graph_module.py:304\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m<eval_with_key>.10 from <eval_with_key>.4 from /home/max/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/nn/modules/container.py:215 in forward:4 in forward:38\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     36\u001b[0m max_pool5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMaxPool5(re_lu5_3);  re_lu5_3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     37\u001b[0m flatten6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFlatten6(max_pool5);  max_pool5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m fc6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflatten6\u001b[49m\u001b[43m)\u001b[49m;  flatten6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     39\u001b[0m re_lu6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU6(fc6)\n\u001b[1;32m     40\u001b[0m fc7 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc7(re_lu6);  re_lu6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x204800 and 25088x4096)"
     ]
    }
   ],
   "source": [
    "classes.calculate_dataset_integration(places_images, integration_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export activations to matlab for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for one taskonomy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 28, 28]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 28, 28]             128\n",
      "             ReLU-32           [-1, 64, 28, 28]               0\n",
      "           Conv2d-33          [-1, 256, 28, 28]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 28, 28]             512\n",
      "        MaxPool2d-35          [-1, 256, 28, 28]               0\n",
      "             ReLU-36          [-1, 256, 28, 28]               0\n",
      "       Bottleneck-37          [-1, 256, 28, 28]               0\n",
      "           Conv2d-38          [-1, 128, 28, 28]          32,768\n",
      "      BatchNorm2d-39          [-1, 128, 28, 28]             256\n",
      "             ReLU-40          [-1, 128, 28, 28]               0\n",
      "           Conv2d-41          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-42          [-1, 128, 28, 28]             256\n",
      "             ReLU-43          [-1, 128, 28, 28]               0\n",
      "           Conv2d-44          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-45          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-46          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-47          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-48          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-49          [-1, 512, 28, 28]               0\n",
      "           Conv2d-50          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-51          [-1, 128, 28, 28]             256\n",
      "             ReLU-52          [-1, 128, 28, 28]               0\n",
      "           Conv2d-53          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
      "             ReLU-55          [-1, 128, 28, 28]               0\n",
      "           Conv2d-56          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-57          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-58          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-59          [-1, 512, 28, 28]               0\n",
      "           Conv2d-60          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "             ReLU-62          [-1, 128, 28, 28]               0\n",
      "           Conv2d-63          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "             ReLU-65          [-1, 128, 28, 28]               0\n",
      "           Conv2d-66          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-67          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-68          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-69          [-1, 512, 28, 28]               0\n",
      "           Conv2d-70          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-71          [-1, 128, 28, 28]             256\n",
      "             ReLU-72          [-1, 128, 28, 28]               0\n",
      "           Conv2d-73          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-74          [-1, 128, 14, 14]             256\n",
      "             ReLU-75          [-1, 128, 14, 14]               0\n",
      "           Conv2d-76          [-1, 512, 14, 14]          65,536\n",
      "      BatchNorm2d-77          [-1, 512, 14, 14]           1,024\n",
      "        MaxPool2d-78          [-1, 512, 14, 14]               0\n",
      "             ReLU-79          [-1, 512, 14, 14]               0\n",
      "       Bottleneck-80          [-1, 512, 14, 14]               0\n",
      "           Conv2d-81          [-1, 256, 14, 14]         131,072\n",
      "      BatchNorm2d-82          [-1, 256, 14, 14]             512\n",
      "             ReLU-83          [-1, 256, 14, 14]               0\n",
      "           Conv2d-84          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-85          [-1, 256, 14, 14]             512\n",
      "             ReLU-86          [-1, 256, 14, 14]               0\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-89         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-90         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-91         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-92         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-93          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-94          [-1, 256, 14, 14]             512\n",
      "             ReLU-95          [-1, 256, 14, 14]               0\n",
      "           Conv2d-96          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-97          [-1, 256, 14, 14]             512\n",
      "             ReLU-98          [-1, 256, 14, 14]               0\n",
      "           Conv2d-99         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-100         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-101         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-102         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-103          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-104          [-1, 256, 14, 14]             512\n",
      "            ReLU-105          [-1, 256, 14, 14]               0\n",
      "          Conv2d-106          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-107          [-1, 256, 14, 14]             512\n",
      "            ReLU-108          [-1, 256, 14, 14]               0\n",
      "          Conv2d-109         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-110         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-111         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-112         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-113          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-114          [-1, 256, 14, 14]             512\n",
      "            ReLU-115          [-1, 256, 14, 14]               0\n",
      "          Conv2d-116          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-117          [-1, 256, 14, 14]             512\n",
      "            ReLU-118          [-1, 256, 14, 14]               0\n",
      "          Conv2d-119         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-120         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-121         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-122         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-123          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-124          [-1, 256, 14, 14]             512\n",
      "            ReLU-125          [-1, 256, 14, 14]               0\n",
      "          Conv2d-126          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-127          [-1, 256, 14, 14]             512\n",
      "            ReLU-128          [-1, 256, 14, 14]               0\n",
      "          Conv2d-129         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-130         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-131         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-132         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-133          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-134          [-1, 256, 14, 14]             512\n",
      "            ReLU-135          [-1, 256, 14, 14]               0\n",
      "          Conv2d-136          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-137          [-1, 256, 14, 14]             512\n",
      "            ReLU-138          [-1, 256, 14, 14]               0\n",
      "          Conv2d-139         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-140         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-141         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-142         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-143          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-144          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-145          [-1, 512, 14, 14]               0\n",
      "          Conv2d-146          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-147          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-148          [-1, 512, 14, 14]               0\n",
      "          Conv2d-149         [-1, 2048, 14, 14]       1,048,576\n",
      "     BatchNorm2d-150         [-1, 2048, 14, 14]           4,096\n",
      "          Conv2d-151         [-1, 2048, 14, 14]       2,097,152\n",
      "     BatchNorm2d-152         [-1, 2048, 14, 14]           4,096\n",
      "            ReLU-153         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-154         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-155          [-1, 512, 14, 14]       1,048,576\n",
      "     BatchNorm2d-156          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-157          [-1, 512, 14, 14]               0\n",
      "          Conv2d-158          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-159          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-160          [-1, 512, 14, 14]               0\n",
      "          Conv2d-161         [-1, 2048, 14, 14]       1,048,576\n",
      "     BatchNorm2d-162         [-1, 2048, 14, 14]           4,096\n",
      "            ReLU-163         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-164         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-165          [-1, 512, 14, 14]       1,048,576\n",
      "     BatchNorm2d-166          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-167          [-1, 512, 14, 14]               0\n",
      "          Conv2d-168          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-169          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-170          [-1, 512, 14, 14]               0\n",
      "          Conv2d-171         [-1, 2048, 14, 14]       1,048,576\n",
      "     BatchNorm2d-172         [-1, 2048, 14, 14]           4,096\n",
      "            ReLU-173         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-174         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-175            [-1, 8, 14, 14]         147,456\n",
      "     BatchNorm2d-176            [-1, 8, 14, 14]              16\n",
      "            ReLU-177            [-1, 8, 14, 14]               0\n",
      "       GroupNorm-178            [-1, 8, 14, 14]               0\n",
      "================================================================\n",
      "Total params: 23,655,504\n",
      "Trainable params: 0\n",
      "Non-trainable params: 23,655,504\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.58\n",
      "Params size (MB): 90.24\n",
      "Estimated Total Size (MB): 377.40\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task = 'autoencoding'\n",
    "\n",
    "VisualPriorRepresentation._load_unloaded_nets([task]) # preload nets\n",
    "net = VisualPriorRepresentation.feature_task_to_net[task] # loads encoder only for representations\n",
    "torchsummary.summary(net, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x   pad   conv1   bn1   relu   maxpool   layer1.0.conv1   layer1.0.bn1   layer1.0.relu   layer1.0.conv2   layer1.0.bn2   layer1.0.relu_1   layer1.0.conv3   layer1.0.bn3   layer1.0.downsample.0   layer1.0.downsample.1   layer1.0.add   layer1.0.relu_2   layer1.1.conv1   layer1.1.bn1   layer1.1.relu   layer1.1.conv2   layer1.1.bn2   layer1.1.relu_1   layer1.1.conv3   layer1.1.bn3   layer1.1.add   layer1.1.relu_2   layer1.2.conv1   layer1.2.bn1   layer1.2.relu   layer1.2.conv2   layer1.2.bn2   layer1.2.relu_1   layer1.2.conv3   layer1.2.bn3   layer1.2.downsample.0   layer1.2.add   layer1.2.relu_2   layer2.0.conv1   layer2.0.bn1   layer2.0.relu   layer2.0.conv2   layer2.0.bn2   layer2.0.relu_1   layer2.0.conv3   layer2.0.bn3   layer2.0.downsample.0   layer2.0.downsample.1   layer2.0.add   layer2.0.relu_2   layer2.1.conv1   layer2.1.bn1   layer2.1.relu   layer2.1.conv2   layer2.1.bn2   layer2.1.relu_1   layer2.1.conv3   layer2.1.bn3   layer2.1.add   layer2.1.relu_2   layer2.2.conv1   layer2.2.bn1   layer2.2.relu   layer2.2.conv2   layer2.2.bn2   layer2.2.relu_1   layer2.2.conv3   layer2.2.bn3   layer2.2.add   layer2.2.relu_2   layer2.3.conv1   layer2.3.bn1   layer2.3.relu   layer2.3.conv2   layer2.3.bn2   layer2.3.relu_1   layer2.3.conv3   layer2.3.bn3   layer2.3.downsample.0   layer2.3.add   layer2.3.relu_2   layer3.0.conv1   layer3.0.bn1   layer3.0.relu   layer3.0.conv2   layer3.0.bn2   layer3.0.relu_1   layer3.0.conv3   layer3.0.bn3   layer3.0.downsample.0   layer3.0.downsample.1   layer3.0.add   layer3.0.relu_2   layer3.1.conv1   layer3.1.bn1   layer3.1.relu   layer3.1.conv2   layer3.1.bn2   layer3.1.relu_1   layer3.1.conv3   layer3.1.bn3   layer3.1.add   layer3.1.relu_2   layer3.2.conv1   layer3.2.bn1   layer3.2.relu   layer3.2.conv2   layer3.2.bn2   layer3.2.relu_1   layer3.2.conv3   layer3.2.bn3   layer3.2.add   layer3.2.relu_2   layer3.3.conv1   layer3.3.bn1   layer3.3.relu   layer3.3.conv2   layer3.3.bn2   layer3.3.relu_1   layer3.3.conv3   layer3.3.bn3   layer3.3.add   layer3.3.relu_2   layer3.4.conv1   layer3.4.bn1   layer3.4.relu   layer3.4.conv2   layer3.4.bn2   layer3.4.relu_1   layer3.4.conv3   layer3.4.bn3   layer3.4.add   layer3.4.relu_2   layer3.5.conv1   layer3.5.bn1   layer3.5.relu   layer3.5.conv2   layer3.5.bn2   layer3.5.relu_1   layer3.5.conv3   layer3.5.bn3   layer3.5.add   layer3.5.relu_2   layer4.0.conv1   layer4.0.bn1   layer4.0.relu   layer4.0.conv2   layer4.0.bn2   layer4.0.relu_1   layer4.0.conv3   layer4.0.bn3   layer4.0.downsample.0   layer4.0.downsample.1   layer4.0.add   layer4.0.relu_2   layer4.1.conv1   layer4.1.bn1   layer4.1.relu   layer4.1.conv2   layer4.1.bn2   layer4.1.relu_1   layer4.1.conv3   layer4.1.bn3   layer4.1.add   layer4.1.relu_2   layer4.2.conv1   layer4.2.bn1   layer4.2.relu   layer4.2.conv2   layer4.2.bn2   layer4.2.relu_1   layer4.2.conv3   layer4.2.bn3   layer4.2.add   layer4.2.relu_2   compress1   compress_bn   relu1   groupnorm   "
     ]
    }
   ],
   "source": [
    "_, eval_nodes = get_graph_node_names(net)\n",
    "for node in eval_nodes:\n",
    "    print(node, end='   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1': 'conv1',\n",
       " 'layer1.0.conv1': 'layer1.0.conv1',\n",
       " 'layer1.0.conv2': 'layer1.0.conv2',\n",
       " 'layer1.0.conv3': 'layer1.0.conv3',\n",
       " 'layer1.1.conv1': 'layer1.1.conv1',\n",
       " 'layer1.1.conv2': 'layer1.1.conv2',\n",
       " 'layer1.1.conv3': 'layer1.1.conv3',\n",
       " 'layer1.2.conv1': 'layer1.2.conv1',\n",
       " 'layer1.2.conv2': 'layer1.2.conv2',\n",
       " 'layer1.2.conv3': 'layer1.2.conv3',\n",
       " 'layer2.0.conv1': 'layer2.0.conv1',\n",
       " 'layer2.0.conv2': 'layer2.0.conv2',\n",
       " 'layer2.0.conv3': 'layer2.0.conv3',\n",
       " 'layer2.1.conv1': 'layer2.1.conv1',\n",
       " 'layer2.1.conv2': 'layer2.1.conv2',\n",
       " 'layer2.1.conv3': 'layer2.1.conv3',\n",
       " 'layer2.2.conv1': 'layer2.2.conv1',\n",
       " 'layer2.2.conv2': 'layer2.2.conv2',\n",
       " 'layer2.2.conv3': 'layer2.2.conv3',\n",
       " 'layer2.3.conv1': 'layer2.3.conv1',\n",
       " 'layer2.3.conv2': 'layer2.3.conv2',\n",
       " 'layer2.3.conv3': 'layer2.3.conv3',\n",
       " 'layer3.0.conv1': 'layer3.0.conv1',\n",
       " 'layer3.0.conv2': 'layer3.0.conv2',\n",
       " 'layer3.0.conv3': 'layer3.0.conv3',\n",
       " 'layer3.1.conv1': 'layer3.1.conv1',\n",
       " 'layer3.1.conv2': 'layer3.1.conv2',\n",
       " 'layer3.1.conv3': 'layer3.1.conv3',\n",
       " 'layer3.2.conv1': 'layer3.2.conv1',\n",
       " 'layer3.2.conv2': 'layer3.2.conv2',\n",
       " 'layer3.2.conv3': 'layer3.2.conv3',\n",
       " 'layer3.3.conv1': 'layer3.3.conv1',\n",
       " 'layer3.3.conv2': 'layer3.3.conv2',\n",
       " 'layer3.3.conv3': 'layer3.3.conv3',\n",
       " 'layer3.4.conv1': 'layer3.4.conv1',\n",
       " 'layer3.4.conv2': 'layer3.4.conv2',\n",
       " 'layer3.4.conv3': 'layer3.4.conv3',\n",
       " 'layer3.5.conv1': 'layer3.5.conv1',\n",
       " 'layer3.5.conv2': 'layer3.5.conv2',\n",
       " 'layer3.5.conv3': 'layer3.5.conv3',\n",
       " 'layer4.0.conv1': 'layer4.0.conv1',\n",
       " 'layer4.0.conv2': 'layer4.0.conv2',\n",
       " 'layer4.0.conv3': 'layer4.0.conv3',\n",
       " 'layer4.1.conv1': 'layer4.1.conv1',\n",
       " 'layer4.1.conv2': 'layer4.1.conv2',\n",
       " 'layer4.1.conv3': 'layer4.1.conv3',\n",
       " 'layer4.2.conv1': 'layer4.2.conv1',\n",
       " 'layer4.2.conv2': 'layer4.2.conv2',\n",
       " 'layer4.2.conv3': 'layer4.2.conv3'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_nodes = { node:node for node in eval_nodes if \"conv\" in node or 'fc' in node}\n",
    "return_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskonomyEncoder(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (layer1): Module(\n",
       "    (0): Module(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Module(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Module(\n",
       "        (0): MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Module(\n",
       "    (0): Module(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Module(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Module(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Module(\n",
       "        (0): MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Module(\n",
       "    (0): Module(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Module(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Module(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): Module(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): Module(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Module(\n",
       "    (0): Module(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Module(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_tweaked = create_feature_extractor(net, return_nodes=return_nodes)\n",
    "net_tweaked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_images = classes.ImageDataset('./data/stimuli_places1', beauty_ratings_path='./behavior/ratings_study1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_calc = classes.IntegrationCalculator(net_tweaked, return_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [14:17,  3.43s/it]\n"
     ]
    }
   ],
   "source": [
    "result = classes.calculate_dataset_integration(places_images, integration_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv1</th>\n",
       "      <th>layer1.0.conv1</th>\n",
       "      <th>layer1.0.conv2</th>\n",
       "      <th>layer1.0.conv3</th>\n",
       "      <th>layer1.1.conv1</th>\n",
       "      <th>layer1.1.conv2</th>\n",
       "      <th>layer1.1.conv3</th>\n",
       "      <th>layer1.2.conv1</th>\n",
       "      <th>layer1.2.conv2</th>\n",
       "      <th>layer1.2.conv3</th>\n",
       "      <th>...</th>\n",
       "      <th>layer3.5.conv3</th>\n",
       "      <th>layer4.0.conv1</th>\n",
       "      <th>layer4.0.conv2</th>\n",
       "      <th>layer4.0.conv3</th>\n",
       "      <th>layer4.1.conv1</th>\n",
       "      <th>layer4.1.conv2</th>\n",
       "      <th>layer4.1.conv3</th>\n",
       "      <th>layer4.2.conv1</th>\n",
       "      <th>layer4.2.conv2</th>\n",
       "      <th>layer4.2.conv3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.901755</td>\n",
       "      <td>-0.927244</td>\n",
       "      <td>-0.906128</td>\n",
       "      <td>-0.892599</td>\n",
       "      <td>-0.924830</td>\n",
       "      <td>-0.857169</td>\n",
       "      <td>-0.839657</td>\n",
       "      <td>-0.941186</td>\n",
       "      <td>-0.896023</td>\n",
       "      <td>-0.852555</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.859965</td>\n",
       "      <td>-0.995464</td>\n",
       "      <td>-0.858975</td>\n",
       "      <td>-0.725659</td>\n",
       "      <td>-0.895166</td>\n",
       "      <td>-0.546067</td>\n",
       "      <td>-0.366636</td>\n",
       "      <td>-0.620608</td>\n",
       "      <td>-0.348664</td>\n",
       "      <td>-0.371284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.977258</td>\n",
       "      <td>-0.979798</td>\n",
       "      <td>-0.977366</td>\n",
       "      <td>-0.970708</td>\n",
       "      <td>-0.973443</td>\n",
       "      <td>-0.941702</td>\n",
       "      <td>-0.918240</td>\n",
       "      <td>-0.974717</td>\n",
       "      <td>-0.968564</td>\n",
       "      <td>-0.947710</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.938060</td>\n",
       "      <td>-0.996411</td>\n",
       "      <td>-0.912282</td>\n",
       "      <td>-0.805249</td>\n",
       "      <td>-0.957089</td>\n",
       "      <td>-0.706170</td>\n",
       "      <td>-0.470574</td>\n",
       "      <td>-0.755524</td>\n",
       "      <td>-0.356730</td>\n",
       "      <td>-0.136613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.985005</td>\n",
       "      <td>-0.988065</td>\n",
       "      <td>-0.982306</td>\n",
       "      <td>-0.978973</td>\n",
       "      <td>-0.980807</td>\n",
       "      <td>-0.959042</td>\n",
       "      <td>-0.937909</td>\n",
       "      <td>-0.980815</td>\n",
       "      <td>-0.977924</td>\n",
       "      <td>-0.965159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.932084</td>\n",
       "      <td>-0.996749</td>\n",
       "      <td>-0.916451</td>\n",
       "      <td>-0.840916</td>\n",
       "      <td>-0.964348</td>\n",
       "      <td>-0.723489</td>\n",
       "      <td>-0.497462</td>\n",
       "      <td>-0.776550</td>\n",
       "      <td>-0.360851</td>\n",
       "      <td>-0.148527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.969972</td>\n",
       "      <td>-0.971745</td>\n",
       "      <td>-0.968256</td>\n",
       "      <td>-0.959294</td>\n",
       "      <td>-0.971802</td>\n",
       "      <td>-0.943989</td>\n",
       "      <td>-0.924279</td>\n",
       "      <td>-0.974445</td>\n",
       "      <td>-0.967046</td>\n",
       "      <td>-0.948057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.903482</td>\n",
       "      <td>-0.997028</td>\n",
       "      <td>-0.915133</td>\n",
       "      <td>-0.862330</td>\n",
       "      <td>-0.956345</td>\n",
       "      <td>-0.715045</td>\n",
       "      <td>-0.582552</td>\n",
       "      <td>-0.792562</td>\n",
       "      <td>-0.507871</td>\n",
       "      <td>-0.420497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.982985</td>\n",
       "      <td>-0.985039</td>\n",
       "      <td>-0.980419</td>\n",
       "      <td>-0.975774</td>\n",
       "      <td>-0.981731</td>\n",
       "      <td>-0.966215</td>\n",
       "      <td>-0.950199</td>\n",
       "      <td>-0.980384</td>\n",
       "      <td>-0.976013</td>\n",
       "      <td>-0.963792</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.902900</td>\n",
       "      <td>-0.996946</td>\n",
       "      <td>-0.916627</td>\n",
       "      <td>-0.884209</td>\n",
       "      <td>-0.963456</td>\n",
       "      <td>-0.755956</td>\n",
       "      <td>-0.626542</td>\n",
       "      <td>-0.805912</td>\n",
       "      <td>-0.571683</td>\n",
       "      <td>-0.499548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>-0.967946</td>\n",
       "      <td>-0.978558</td>\n",
       "      <td>-0.971832</td>\n",
       "      <td>-0.965229</td>\n",
       "      <td>-0.966798</td>\n",
       "      <td>-0.933618</td>\n",
       "      <td>-0.914758</td>\n",
       "      <td>-0.967229</td>\n",
       "      <td>-0.955363</td>\n",
       "      <td>-0.933746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926590</td>\n",
       "      <td>-0.995430</td>\n",
       "      <td>-0.902838</td>\n",
       "      <td>-0.806128</td>\n",
       "      <td>-0.947222</td>\n",
       "      <td>-0.727485</td>\n",
       "      <td>-0.504238</td>\n",
       "      <td>-0.768200</td>\n",
       "      <td>-0.392083</td>\n",
       "      <td>-0.184156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>-0.971098</td>\n",
       "      <td>-0.974567</td>\n",
       "      <td>-0.971881</td>\n",
       "      <td>-0.963860</td>\n",
       "      <td>-0.971524</td>\n",
       "      <td>-0.946957</td>\n",
       "      <td>-0.928143</td>\n",
       "      <td>-0.972739</td>\n",
       "      <td>-0.964131</td>\n",
       "      <td>-0.943081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.925275</td>\n",
       "      <td>-0.996563</td>\n",
       "      <td>-0.914013</td>\n",
       "      <td>-0.855937</td>\n",
       "      <td>-0.954991</td>\n",
       "      <td>-0.736623</td>\n",
       "      <td>-0.608247</td>\n",
       "      <td>-0.798507</td>\n",
       "      <td>-0.543984</td>\n",
       "      <td>-0.475614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>-0.970213</td>\n",
       "      <td>-0.975484</td>\n",
       "      <td>-0.968955</td>\n",
       "      <td>-0.964136</td>\n",
       "      <td>-0.970212</td>\n",
       "      <td>-0.949807</td>\n",
       "      <td>-0.937206</td>\n",
       "      <td>-0.966894</td>\n",
       "      <td>-0.957912</td>\n",
       "      <td>-0.952441</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.924776</td>\n",
       "      <td>-0.993317</td>\n",
       "      <td>-0.912605</td>\n",
       "      <td>-0.901262</td>\n",
       "      <td>-0.942384</td>\n",
       "      <td>-0.802596</td>\n",
       "      <td>-0.611354</td>\n",
       "      <td>-0.816946</td>\n",
       "      <td>-0.436379</td>\n",
       "      <td>-0.208226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>-0.991683</td>\n",
       "      <td>-0.993783</td>\n",
       "      <td>-0.991791</td>\n",
       "      <td>-0.989363</td>\n",
       "      <td>-0.987538</td>\n",
       "      <td>-0.974684</td>\n",
       "      <td>-0.956181</td>\n",
       "      <td>-0.986093</td>\n",
       "      <td>-0.987357</td>\n",
       "      <td>-0.976262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.924651</td>\n",
       "      <td>-0.997633</td>\n",
       "      <td>-0.923652</td>\n",
       "      <td>-0.856157</td>\n",
       "      <td>-0.972104</td>\n",
       "      <td>-0.729208</td>\n",
       "      <td>-0.493144</td>\n",
       "      <td>-0.781738</td>\n",
       "      <td>-0.362910</td>\n",
       "      <td>-0.120459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>-0.986087</td>\n",
       "      <td>-0.985298</td>\n",
       "      <td>-0.984929</td>\n",
       "      <td>-0.979691</td>\n",
       "      <td>-0.986239</td>\n",
       "      <td>-0.975361</td>\n",
       "      <td>-0.969088</td>\n",
       "      <td>-0.987772</td>\n",
       "      <td>-0.984247</td>\n",
       "      <td>-0.973219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.894974</td>\n",
       "      <td>-0.997936</td>\n",
       "      <td>-0.921355</td>\n",
       "      <td>-0.899193</td>\n",
       "      <td>-0.969977</td>\n",
       "      <td>-0.780987</td>\n",
       "      <td>-0.716732</td>\n",
       "      <td>-0.824350</td>\n",
       "      <td>-0.700911</td>\n",
       "      <td>-0.661872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows  49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        conv1  layer1.0.conv1  layer1.0.conv2  layer1.0.conv3  layer1.1.conv1  \\\n",
       "0   -0.901755       -0.927244       -0.906128       -0.892599       -0.924830   \n",
       "1   -0.977258       -0.979798       -0.977366       -0.970708       -0.973443   \n",
       "2   -0.985005       -0.988065       -0.982306       -0.978973       -0.980807   \n",
       "3   -0.969972       -0.971745       -0.968256       -0.959294       -0.971802   \n",
       "4   -0.982985       -0.985039       -0.980419       -0.975774       -0.981731   \n",
       "..        ...             ...             ...             ...             ...   \n",
       "245 -0.967946       -0.978558       -0.971832       -0.965229       -0.966798   \n",
       "246 -0.971098       -0.974567       -0.971881       -0.963860       -0.971524   \n",
       "247 -0.970213       -0.975484       -0.968955       -0.964136       -0.970212   \n",
       "248 -0.991683       -0.993783       -0.991791       -0.989363       -0.987538   \n",
       "249 -0.986087       -0.985298       -0.984929       -0.979691       -0.986239   \n",
       "\n",
       "     layer1.1.conv2  layer1.1.conv3  layer1.2.conv1  layer1.2.conv2  \\\n",
       "0         -0.857169       -0.839657       -0.941186       -0.896023   \n",
       "1         -0.941702       -0.918240       -0.974717       -0.968564   \n",
       "2         -0.959042       -0.937909       -0.980815       -0.977924   \n",
       "3         -0.943989       -0.924279       -0.974445       -0.967046   \n",
       "4         -0.966215       -0.950199       -0.980384       -0.976013   \n",
       "..              ...             ...             ...             ...   \n",
       "245       -0.933618       -0.914758       -0.967229       -0.955363   \n",
       "246       -0.946957       -0.928143       -0.972739       -0.964131   \n",
       "247       -0.949807       -0.937206       -0.966894       -0.957912   \n",
       "248       -0.974684       -0.956181       -0.986093       -0.987357   \n",
       "249       -0.975361       -0.969088       -0.987772       -0.984247   \n",
       "\n",
       "     layer1.2.conv3  ...  layer3.5.conv3  layer4.0.conv1  layer4.0.conv2  \\\n",
       "0         -0.852555  ...       -0.859965       -0.995464       -0.858975   \n",
       "1         -0.947710  ...       -0.938060       -0.996411       -0.912282   \n",
       "2         -0.965159  ...       -0.932084       -0.996749       -0.916451   \n",
       "3         -0.948057  ...       -0.903482       -0.997028       -0.915133   \n",
       "4         -0.963792  ...       -0.902900       -0.996946       -0.916627   \n",
       "..              ...  ...             ...             ...             ...   \n",
       "245       -0.933746  ...       -0.926590       -0.995430       -0.902838   \n",
       "246       -0.943081  ...       -0.925275       -0.996563       -0.914013   \n",
       "247       -0.952441  ...       -0.924776       -0.993317       -0.912605   \n",
       "248       -0.976262  ...       -0.924651       -0.997633       -0.923652   \n",
       "249       -0.973219  ...       -0.894974       -0.997936       -0.921355   \n",
       "\n",
       "     layer4.0.conv3  layer4.1.conv1  layer4.1.conv2  layer4.1.conv3  \\\n",
       "0         -0.725659       -0.895166       -0.546067       -0.366636   \n",
       "1         -0.805249       -0.957089       -0.706170       -0.470574   \n",
       "2         -0.840916       -0.964348       -0.723489       -0.497462   \n",
       "3         -0.862330       -0.956345       -0.715045       -0.582552   \n",
       "4         -0.884209       -0.963456       -0.755956       -0.626542   \n",
       "..              ...             ...             ...             ...   \n",
       "245       -0.806128       -0.947222       -0.727485       -0.504238   \n",
       "246       -0.855937       -0.954991       -0.736623       -0.608247   \n",
       "247       -0.901262       -0.942384       -0.802596       -0.611354   \n",
       "248       -0.856157       -0.972104       -0.729208       -0.493144   \n",
       "249       -0.899193       -0.969977       -0.780987       -0.716732   \n",
       "\n",
       "     layer4.2.conv1  layer4.2.conv2  layer4.2.conv3  \n",
       "0         -0.620608       -0.348664       -0.371284  \n",
       "1         -0.755524       -0.356730       -0.136613  \n",
       "2         -0.776550       -0.360851       -0.148527  \n",
       "3         -0.792562       -0.507871       -0.420497  \n",
       "4         -0.805912       -0.571683       -0.499548  \n",
       "..              ...             ...             ...  \n",
       "245       -0.768200       -0.392083       -0.184156  \n",
       "246       -0.798507       -0.543984       -0.475614  \n",
       "247       -0.816946       -0.436379       -0.208226  \n",
       "248       -0.781738       -0.362910       -0.120459  \n",
       "249       -0.824350       -0.700911       -0.661872  \n",
       "\n",
       "[250 rows x 49 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for all taskonomy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_taskonomy_encoder_integration_analysis(task, places_images):\n",
    "    VisualPriorRepresentation._load_unloaded_nets([task])\n",
    "    net = VisualPriorRepresentation.feature_task_to_net[task]\n",
    "\n",
    "    _, eval_nodes = get_graph_node_names(net)\n",
    "    return_nodes = { node:node for node in eval_nodes if \"conv\" in node or 'fc' in node}\n",
    "    net_tweaked = create_feature_extractor(net, return_nodes=return_nodes)\n",
    "\n",
    "    integration_calc = classes.IntegrationCalculator(net_tweaked, return_nodes)\n",
    "    return classes.calculate_dataset_integration(places_images, integration_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ('autoencoding','depth_euclidean','jigsaw','reshading',\n",
    "         'colorization', 'edge_occlusion','keypoints2d','room_layout',\n",
    "         'curvature','edge_texture','keypoints3d','segment_unsup2d',\n",
    "         'class_object','egomotion','nonfixated_pose','segment_unsup25d',\n",
    "         'class_scene','fixated_pose','normal','segment_semantic',\n",
    "         'denoising','inpainting','point_matching','vanishing_point')\n",
    "\n",
    "places_images = classes.ImageDataset('./data/stimuli_places1', beauty_ratings_path='./behavior/ratings_study1.csv')\n",
    "\n",
    "mdict = {}\n",
    "for task in tasks:\n",
    "    task_result = run_taskonomy_encoder_integration_analysis(task, places_images)\n",
    "    task_result.to_csv(os.path.join('results', task, task + '.csv'))\n",
    "    #mdict[task] = run_taskonomy_encoder_integration_analysis(task, places_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#savemat(\"taskonomy_integration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
