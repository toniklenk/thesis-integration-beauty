{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes_taskonomy import ImageDataset, calculate_dataset_correlation, correlate_integration_beauty\n",
    "# modified visualpriors library\n",
    "from transforms import VisualPrior\n",
    "from transforms import VisualPriorRepresentation, VisualPriorPredictedLabel\n",
    "from taskonomy_network import TaskonomyEncoder, TaskonomyDecoder\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import warnings\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from math import isnan\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchsummary\n",
    "\n",
    "from PIL import Image \n",
    "from scipy.stats import pearsonr, PearsonRConstantInputWarning\n",
    "from scipy.io import savemat, loadmat\n",
    "\n",
    "import torch\n",
    "import torch.utils.model_zoo # required to load nets \n",
    "import torchvision.transforms.functional as TF\n",
    "from torch import nn \n",
    "import torchvision.models\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.19\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Img.open+TF.to_tensor do a absoloute(/255) or Min-Max scale ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all images\n",
    "img_dir = \"./data/stimuli_places1\"\n",
    "\n",
    "dir_img_list = list(f for f in os.listdir(img_dir)\n",
    "                            if os.path.isfile(os.path.join(img_dir, f)) and f.endswith('.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmin, lmax = [],[]\n",
    "for image_name in dir_img_list:\n",
    "    img = TF.to_tensor(Image.open(os.path.join(img_dir, image_name)))\n",
    "    lmin.append(img.min())\n",
    "    lmax.append(img.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7412)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(lmax).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(lmin).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Img.open+TF.to_tensor does a absoloute scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare matlab image import to Image.open import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_matlab = torch.tensor(loadmat('1153_unaltered_matlab.mat')['im']).permute([2, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pytorch = TF.to_tensor(Image.open('./data/stimuli_places1/Places365_val_00001153.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 683, 512])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_matlab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 683, 512])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_pytorch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0275, 0.0627, 0.0667,  ..., 0.6078, 0.7412, 0.6549],\n",
       "         [0.0667, 0.0667, 0.0431,  ..., 0.4706, 0.5529, 0.4275],\n",
       "         [0.1882, 0.1490, 0.1020,  ..., 0.3294, 0.3725, 0.2549],\n",
       "         ...,\n",
       "         [0.0314, 0.0275, 0.0275,  ..., 0.0824, 0.0784, 0.0745],\n",
       "         [0.0314, 0.0275, 0.0275,  ..., 0.0824, 0.0784, 0.0745],\n",
       "         [0.0314, 0.0275, 0.0275,  ..., 0.0824, 0.0784, 0.0745]],\n",
       "\n",
       "        [[0.0157, 0.0549, 0.0588,  ..., 0.4588, 0.6157, 0.5333],\n",
       "         [0.0588, 0.0588, 0.0353,  ..., 0.3294, 0.4235, 0.3137],\n",
       "         [0.1922, 0.1529, 0.1059,  ..., 0.2039, 0.2588, 0.1451],\n",
       "         ...,\n",
       "         [0.0549, 0.0510, 0.0510,  ..., 0.2235, 0.2196, 0.2157],\n",
       "         [0.0549, 0.0510, 0.0510,  ..., 0.2235, 0.2196, 0.2157],\n",
       "         [0.0549, 0.0510, 0.0510,  ..., 0.2235, 0.2196, 0.2157]],\n",
       "\n",
       "        [[0.0000, 0.0078, 0.0078,  ..., 0.3686, 0.5333, 0.4549],\n",
       "         [0.0118, 0.0118, 0.0000,  ..., 0.2431, 0.3490, 0.2431],\n",
       "         [0.1373, 0.0902, 0.0353,  ..., 0.1216, 0.1961, 0.0902],\n",
       "         ...,\n",
       "         [0.0157, 0.0118, 0.0118,  ..., 0.1765, 0.1725, 0.1686],\n",
       "         [0.0157, 0.0118, 0.0118,  ..., 0.1765, 0.1725, 0.1686],\n",
       "         [0.0157, 0.0118, 0.0118,  ..., 0.1765, 0.1725, 0.1686]]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0275, 0.0627, 0.0667,  ..., 0.6078, 0.7412, 0.6549],\n",
       "         [0.0588, 0.0667, 0.0431,  ..., 0.4706, 0.5490, 0.4275],\n",
       "         [0.1882, 0.1490, 0.1020,  ..., 0.3294, 0.3725, 0.2510],\n",
       "         ...,\n",
       "         [0.0314, 0.0275, 0.0275,  ..., 0.0824, 0.0784, 0.0745],\n",
       "         [0.0314, 0.0275, 0.0275,  ..., 0.0824, 0.0784, 0.0745],\n",
       "         [0.0314, 0.0275, 0.0275,  ..., 0.0824, 0.0784, 0.0745]],\n",
       "\n",
       "        [[0.0157, 0.0510, 0.0588,  ..., 0.4588, 0.6157, 0.5333],\n",
       "         [0.0588, 0.0588, 0.0353,  ..., 0.3294, 0.4275, 0.3137],\n",
       "         [0.1882, 0.1529, 0.1059,  ..., 0.2000, 0.2588, 0.1490],\n",
       "         ...,\n",
       "         [0.0549, 0.0510, 0.0510,  ..., 0.2235, 0.2196, 0.2157],\n",
       "         [0.0549, 0.0510, 0.0510,  ..., 0.2235, 0.2196, 0.2157],\n",
       "         [0.0549, 0.0510, 0.0510,  ..., 0.2235, 0.2196, 0.2157]],\n",
       "\n",
       "        [[0.0000, 0.0157, 0.0118,  ..., 0.3686, 0.5333, 0.4549],\n",
       "         [0.0196, 0.0118, 0.0000,  ..., 0.2431, 0.3490, 0.2431],\n",
       "         [0.1412, 0.0980, 0.0431,  ..., 0.1255, 0.1961, 0.0902],\n",
       "         ...,\n",
       "         [0.0157, 0.0118, 0.0118,  ..., 0.1765, 0.1725, 0.1686],\n",
       "         [0.0157, 0.0118, 0.0118,  ..., 0.1765, 0.1725, 0.1686],\n",
       "         [0.0157, 0.0118, 0.0118,  ..., 0.1765, 0.1725, 0.1686]]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0103)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img_matlab-img_pytorch).abs().sum() / (256*256*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 24.440285 ,  22.75106  ,  11.813852 ],\n",
       "        [ 28.500277 ,  27.750185 ,  10.516175 ],\n",
       "        [ 46.8996   ,  46.354485 ,  20.093699 ],\n",
       "        ...,\n",
       "        [153.51295  , 112.667366 ,  73.45321  ],\n",
       "        [114.17797  ,  74.978455 ,  48.79689  ],\n",
       "        [128.2486   ,  97.46462  ,  79.03427  ]],\n",
       "\n",
       "       [[ 58.05664  ,  61.188354 ,  38.276947 ],\n",
       "        [ 54.278408 ,  57.572754 ,  31.45102  ],\n",
       "        [ 52.65642  ,  55.115265 ,  25.720566 ],\n",
       "        ...,\n",
       "        [120.24372  ,  86.13118  ,  58.90142  ],\n",
       "        [ 59.342533 ,  29.822533 ,  15.940515 ],\n",
       "        [ 42.107304 ,  22.485958 ,  14.650325 ]],\n",
       "\n",
       "       [[ 36.602886 ,  44.835506 ,  11.999634 ],\n",
       "        [ 41.501434 ,  49.640537 ,  15.932427 ],\n",
       "        [ 40.36918  ,  46.6992   ,  15.063794 ],\n",
       "        ...,\n",
       "        [149.50587  , 125.88573  , 112.681854 ],\n",
       "        [ 44.91447  ,  27.869555 ,  25.124537 ],\n",
       "        [  7.946582 ,   2.764554 ,   4.4361887]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  5.589234 ,  10.654429 ,   4.7645097],\n",
       "        [  5.1870656,  10.241853 ,   4.3630643],\n",
       "        [  4.966198 ,  10.025234 ,   4.131884 ],\n",
       "        ...,\n",
       "        [ 24.30002  ,  60.362453 ,  48.44695  ],\n",
       "        [ 24.294035 ,  60.36241  ,  48.43947  ],\n",
       "        [ 22.155594 ,  58.21959  ,  46.30638  ]],\n",
       "\n",
       "       [[  5.020131 ,  10.07428  ,   3.095491 ],\n",
       "        [  6.1730447,  11.240618 ,   4.445555 ],\n",
       "        [  8.622787 ,  13.654462 ,   6.6589384],\n",
       "        ...,\n",
       "        [ 22.70262  ,  58.70262  ,  46.70262  ],\n",
       "        [ 20.511572 ,  56.511574 ,  44.511574 ],\n",
       "        [ 19.557453 ,  55.557453 ,  43.557453 ]],\n",
       "\n",
       "       [[  7.375475 ,  13.255985 ,   3.4189425],\n",
       "        [  7.1908016,  13.093838 ,   3.2278614],\n",
       "        [  9.188443 ,  15.031223 ,   5.2229247],\n",
       "        ...,\n",
       "        [ 23.735758 ,  59.735756 ,  47.735756 ],\n",
       "        [ 21.83632  ,  57.83632  ,  45.83632  ],\n",
       "        [ 19.712322 ,  55.712322 ,  43.712322 ]]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadmat(\"data/stimuli_places1_resized/Places365_val_00001153.mat\")['im']\n",
    "import subprocess\n",
    "subprocess.call(\"curl -O https://raw.githubusercontent.com/StanfordVL/taskonomy/master/taskbank/assets/test.png\", shell=True)\n",
    "img = Image.open('./data/stimuli_places1/Places365_val_00001153.jpg')\n",
    "TF.to_tensor(img)\n",
    "\n",
    "img = TF.to_tensor(TF.resize(img, 256)) * 2 - 1 # resize\n",
    "img = img.unsqueeze_(0)\n",
    "\n",
    "# load model\n",
    "task = 'autoencoding'\n",
    "\n",
    "VisualPriorRepresentation._load_unloaded_nets([task]) # preload nets\n",
    "net = VisualPriorRepresentation.feature_task_to_net[task] # loads encoder only for representations\n",
    "torchsummary.summary(net, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale to [-1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build activation extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 128, 128]             128\n",
      "              ReLU-3         [-1, 64, 128, 128]               0\n",
      "         MaxPool2d-4           [-1, 64, 64, 64]               0\n",
      "            Conv2d-5           [-1, 64, 64, 64]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 64, 64]             128\n",
      "              ReLU-7           [-1, 64, 64, 64]               0\n",
      "            Conv2d-8           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 64, 64]             128\n",
      "             ReLU-10           [-1, 64, 64, 64]               0\n",
      "           Conv2d-11          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 64, 64]             512\n",
      "           Conv2d-13          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 64, 64]             512\n",
      "             ReLU-15          [-1, 256, 64, 64]               0\n",
      "       Bottleneck-16          [-1, 256, 64, 64]               0\n",
      "           Conv2d-17           [-1, 64, 64, 64]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 64, 64]             128\n",
      "             ReLU-19           [-1, 64, 64, 64]               0\n",
      "           Conv2d-20           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 64, 64]             128\n",
      "             ReLU-22           [-1, 64, 64, 64]               0\n",
      "           Conv2d-23          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 64, 64]             512\n",
      "             ReLU-25          [-1, 256, 64, 64]               0\n",
      "       Bottleneck-26          [-1, 256, 64, 64]               0\n",
      "           Conv2d-27           [-1, 64, 64, 64]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 64, 64]             128\n",
      "             ReLU-29           [-1, 64, 64, 64]               0\n",
      "           Conv2d-30           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 32, 32]             128\n",
      "             ReLU-32           [-1, 64, 32, 32]               0\n",
      "           Conv2d-33          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 32, 32]             512\n",
      "        MaxPool2d-35          [-1, 256, 32, 32]               0\n",
      "             ReLU-36          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-37          [-1, 256, 32, 32]               0\n",
      "           Conv2d-38          [-1, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-39          [-1, 128, 32, 32]             256\n",
      "             ReLU-40          [-1, 128, 32, 32]               0\n",
      "           Conv2d-41          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-42          [-1, 128, 32, 32]             256\n",
      "             ReLU-43          [-1, 128, 32, 32]               0\n",
      "           Conv2d-44          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-45          [-1, 512, 32, 32]           1,024\n",
      "           Conv2d-46          [-1, 512, 32, 32]         131,072\n",
      "      BatchNorm2d-47          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-48          [-1, 512, 32, 32]               0\n",
      "       Bottleneck-49          [-1, 512, 32, 32]               0\n",
      "           Conv2d-50          [-1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-51          [-1, 128, 32, 32]             256\n",
      "             ReLU-52          [-1, 128, 32, 32]               0\n",
      "           Conv2d-53          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-54          [-1, 128, 32, 32]             256\n",
      "             ReLU-55          [-1, 128, 32, 32]               0\n",
      "           Conv2d-56          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-57          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-58          [-1, 512, 32, 32]               0\n",
      "       Bottleneck-59          [-1, 512, 32, 32]               0\n",
      "           Conv2d-60          [-1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-61          [-1, 128, 32, 32]             256\n",
      "             ReLU-62          [-1, 128, 32, 32]               0\n",
      "           Conv2d-63          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-64          [-1, 128, 32, 32]             256\n",
      "             ReLU-65          [-1, 128, 32, 32]               0\n",
      "           Conv2d-66          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-67          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-68          [-1, 512, 32, 32]               0\n",
      "       Bottleneck-69          [-1, 512, 32, 32]               0\n",
      "           Conv2d-70          [-1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-71          [-1, 128, 32, 32]             256\n",
      "             ReLU-72          [-1, 128, 32, 32]               0\n",
      "           Conv2d-73          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-74          [-1, 128, 16, 16]             256\n",
      "             ReLU-75          [-1, 128, 16, 16]               0\n",
      "           Conv2d-76          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-77          [-1, 512, 16, 16]           1,024\n",
      "        MaxPool2d-78          [-1, 512, 16, 16]               0\n",
      "             ReLU-79          [-1, 512, 16, 16]               0\n",
      "       Bottleneck-80          [-1, 512, 16, 16]               0\n",
      "           Conv2d-81          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-82          [-1, 256, 16, 16]             512\n",
      "             ReLU-83          [-1, 256, 16, 16]               0\n",
      "           Conv2d-84          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-85          [-1, 256, 16, 16]             512\n",
      "             ReLU-86          [-1, 256, 16, 16]               0\n",
      "           Conv2d-87         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-88         [-1, 1024, 16, 16]           2,048\n",
      "           Conv2d-89         [-1, 1024, 16, 16]         524,288\n",
      "      BatchNorm2d-90         [-1, 1024, 16, 16]           2,048\n",
      "             ReLU-91         [-1, 1024, 16, 16]               0\n",
      "       Bottleneck-92         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-93          [-1, 256, 16, 16]         262,144\n",
      "      BatchNorm2d-94          [-1, 256, 16, 16]             512\n",
      "             ReLU-95          [-1, 256, 16, 16]               0\n",
      "           Conv2d-96          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-97          [-1, 256, 16, 16]             512\n",
      "             ReLU-98          [-1, 256, 16, 16]               0\n",
      "           Conv2d-99         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-100         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-101         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-102         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-103          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-104          [-1, 256, 16, 16]             512\n",
      "            ReLU-105          [-1, 256, 16, 16]               0\n",
      "          Conv2d-106          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-107          [-1, 256, 16, 16]             512\n",
      "            ReLU-108          [-1, 256, 16, 16]               0\n",
      "          Conv2d-109         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-110         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-111         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-112         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-113          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-114          [-1, 256, 16, 16]             512\n",
      "            ReLU-115          [-1, 256, 16, 16]               0\n",
      "          Conv2d-116          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-117          [-1, 256, 16, 16]             512\n",
      "            ReLU-118          [-1, 256, 16, 16]               0\n",
      "          Conv2d-119         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-120         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-121         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-122         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-123          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-124          [-1, 256, 16, 16]             512\n",
      "            ReLU-125          [-1, 256, 16, 16]               0\n",
      "          Conv2d-126          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-127          [-1, 256, 16, 16]             512\n",
      "            ReLU-128          [-1, 256, 16, 16]               0\n",
      "          Conv2d-129         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-130         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-131         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-132         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-133          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-134          [-1, 256, 16, 16]             512\n",
      "            ReLU-135          [-1, 256, 16, 16]               0\n",
      "          Conv2d-136          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-137          [-1, 256, 16, 16]             512\n",
      "            ReLU-138          [-1, 256, 16, 16]               0\n",
      "          Conv2d-139         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-140         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-141         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-142         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-143          [-1, 512, 16, 16]         524,288\n",
      "     BatchNorm2d-144          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-145          [-1, 512, 16, 16]               0\n",
      "          Conv2d-146          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-147          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-148          [-1, 512, 16, 16]               0\n",
      "          Conv2d-149         [-1, 2048, 16, 16]       1,048,576\n",
      "     BatchNorm2d-150         [-1, 2048, 16, 16]           4,096\n",
      "          Conv2d-151         [-1, 2048, 16, 16]       2,097,152\n",
      "     BatchNorm2d-152         [-1, 2048, 16, 16]           4,096\n",
      "            ReLU-153         [-1, 2048, 16, 16]               0\n",
      "      Bottleneck-154         [-1, 2048, 16, 16]               0\n",
      "          Conv2d-155          [-1, 512, 16, 16]       1,048,576\n",
      "     BatchNorm2d-156          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-157          [-1, 512, 16, 16]               0\n",
      "          Conv2d-158          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-159          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-160          [-1, 512, 16, 16]               0\n",
      "          Conv2d-161         [-1, 2048, 16, 16]       1,048,576\n",
      "     BatchNorm2d-162         [-1, 2048, 16, 16]           4,096\n",
      "            ReLU-163         [-1, 2048, 16, 16]               0\n",
      "      Bottleneck-164         [-1, 2048, 16, 16]               0\n",
      "          Conv2d-165          [-1, 512, 16, 16]       1,048,576\n",
      "     BatchNorm2d-166          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-167          [-1, 512, 16, 16]               0\n",
      "          Conv2d-168          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-169          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-170          [-1, 512, 16, 16]               0\n",
      "          Conv2d-171         [-1, 2048, 16, 16]       1,048,576\n",
      "     BatchNorm2d-172         [-1, 2048, 16, 16]           4,096\n",
      "            ReLU-173         [-1, 2048, 16, 16]               0\n",
      "      Bottleneck-174         [-1, 2048, 16, 16]               0\n",
      "          Conv2d-175            [-1, 8, 16, 16]         147,456\n",
      "     BatchNorm2d-176            [-1, 8, 16, 16]              16\n",
      "            ReLU-177            [-1, 8, 16, 16]               0\n",
      "       GroupNorm-178            [-1, 8, 16, 16]               0\n",
      "================================================================\n",
      "Total params: 23,655,504\n",
      "Trainable params: 0\n",
      "Non-trainable params: 23,655,504\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 374.31\n",
      "Params size (MB): 90.24\n",
      "Estimated Total Size (MB): 465.30\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "task = 'autoencoding'\n",
    "VisualPriorRepresentation._load_unloaded_nets([task])\n",
    "net = VisualPriorRepresentation.feature_task_to_net[task]\n",
    "torchsummary.summary(net, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Layers to extract activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, eval_nodes = get_graph_node_names(net)\n",
    "return_nodes = {node:node for node in eval_nodes if \"conv\" in node or \"fc\" in node}\n",
    "activation_extractor = create_feature_extractor(net, return_nodes=return_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ImageDatasetTaskonomy(os.path.join('data','stimuli_places1','untransformed','scale8'), beauty_ratings_path='./behavior/ratings_study1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "integration_ratings = calculate_dataset_integration(data, activation_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv1</th>\n",
       "      <th>layer1.0.conv1</th>\n",
       "      <th>layer1.0.conv2</th>\n",
       "      <th>layer1.0.conv3</th>\n",
       "      <th>layer1.1.conv1</th>\n",
       "      <th>layer1.1.conv2</th>\n",
       "      <th>layer1.1.conv3</th>\n",
       "      <th>layer1.2.conv1</th>\n",
       "      <th>layer1.2.conv2</th>\n",
       "      <th>layer1.2.conv3</th>\n",
       "      <th>...</th>\n",
       "      <th>layer3.5.conv3</th>\n",
       "      <th>layer4.0.conv1</th>\n",
       "      <th>layer4.0.conv2</th>\n",
       "      <th>layer4.0.conv3</th>\n",
       "      <th>layer4.1.conv1</th>\n",
       "      <th>layer4.1.conv2</th>\n",
       "      <th>layer4.1.conv3</th>\n",
       "      <th>layer4.2.conv1</th>\n",
       "      <th>layer4.2.conv2</th>\n",
       "      <th>layer4.2.conv3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.936642</td>\n",
       "      <td>-0.934466</td>\n",
       "      <td>-0.935969</td>\n",
       "      <td>-0.904005</td>\n",
       "      <td>-0.935814</td>\n",
       "      <td>-0.931759</td>\n",
       "      <td>-0.918483</td>\n",
       "      <td>-0.911972</td>\n",
       "      <td>-0.890295</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137930</td>\n",
       "      <td>-0.770316</td>\n",
       "      <td>-0.453550</td>\n",
       "      <td>-0.456116</td>\n",
       "      <td>-0.594658</td>\n",
       "      <td>-0.359765</td>\n",
       "      <td>-0.178945</td>\n",
       "      <td>-0.347093</td>\n",
       "      <td>-0.281436</td>\n",
       "      <td>-0.135682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.999955</td>\n",
       "      <td>-0.963097</td>\n",
       "      <td>-0.884371</td>\n",
       "      <td>-0.890870</td>\n",
       "      <td>-0.906541</td>\n",
       "      <td>-0.925186</td>\n",
       "      <td>-0.910882</td>\n",
       "      <td>-0.925319</td>\n",
       "      <td>-0.903241</td>\n",
       "      <td>-0.878101</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.920808</td>\n",
       "      <td>-0.990737</td>\n",
       "      <td>-0.896368</td>\n",
       "      <td>-0.936358</td>\n",
       "      <td>-0.929399</td>\n",
       "      <td>-0.823115</td>\n",
       "      <td>-0.915983</td>\n",
       "      <td>-0.912726</td>\n",
       "      <td>-0.839880</td>\n",
       "      <td>-0.905891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.999948</td>\n",
       "      <td>-0.959299</td>\n",
       "      <td>-0.862562</td>\n",
       "      <td>-0.866453</td>\n",
       "      <td>-0.903425</td>\n",
       "      <td>-0.888381</td>\n",
       "      <td>-0.885857</td>\n",
       "      <td>-0.918011</td>\n",
       "      <td>-0.901663</td>\n",
       "      <td>-0.846007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.887986</td>\n",
       "      <td>-0.991643</td>\n",
       "      <td>-0.874940</td>\n",
       "      <td>-0.924446</td>\n",
       "      <td>-0.919121</td>\n",
       "      <td>-0.789649</td>\n",
       "      <td>-0.890146</td>\n",
       "      <td>-0.913709</td>\n",
       "      <td>-0.841277</td>\n",
       "      <td>-0.894605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.952015</td>\n",
       "      <td>-0.918668</td>\n",
       "      <td>-0.916533</td>\n",
       "      <td>-0.912042</td>\n",
       "      <td>-0.931676</td>\n",
       "      <td>-0.923608</td>\n",
       "      <td>-0.928480</td>\n",
       "      <td>-0.914450</td>\n",
       "      <td>-0.888074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214854</td>\n",
       "      <td>-0.879364</td>\n",
       "      <td>-0.609108</td>\n",
       "      <td>-0.485275</td>\n",
       "      <td>-0.752628</td>\n",
       "      <td>-0.663677</td>\n",
       "      <td>-0.454453</td>\n",
       "      <td>-0.683164</td>\n",
       "      <td>-0.776617</td>\n",
       "      <td>-0.865294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.999962</td>\n",
       "      <td>-0.942360</td>\n",
       "      <td>-0.892700</td>\n",
       "      <td>-0.889042</td>\n",
       "      <td>-0.909401</td>\n",
       "      <td>-0.898860</td>\n",
       "      <td>-0.882733</td>\n",
       "      <td>-0.924383</td>\n",
       "      <td>-0.911258</td>\n",
       "      <td>-0.859058</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.867012</td>\n",
       "      <td>-0.988206</td>\n",
       "      <td>-0.849169</td>\n",
       "      <td>-0.868761</td>\n",
       "      <td>-0.908572</td>\n",
       "      <td>-0.765086</td>\n",
       "      <td>-0.820054</td>\n",
       "      <td>-0.886940</td>\n",
       "      <td>-0.779974</td>\n",
       "      <td>-0.780867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>-0.999974</td>\n",
       "      <td>-0.956059</td>\n",
       "      <td>-0.908246</td>\n",
       "      <td>-0.908545</td>\n",
       "      <td>-0.915434</td>\n",
       "      <td>-0.922431</td>\n",
       "      <td>-0.911444</td>\n",
       "      <td>-0.918918</td>\n",
       "      <td>-0.906877</td>\n",
       "      <td>-0.870916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.890650</td>\n",
       "      <td>-0.986374</td>\n",
       "      <td>-0.857403</td>\n",
       "      <td>-0.919065</td>\n",
       "      <td>-0.909058</td>\n",
       "      <td>-0.791113</td>\n",
       "      <td>-0.905559</td>\n",
       "      <td>-0.905908</td>\n",
       "      <td>-0.812710</td>\n",
       "      <td>-0.891740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>-0.999966</td>\n",
       "      <td>-0.955644</td>\n",
       "      <td>-0.913804</td>\n",
       "      <td>-0.911800</td>\n",
       "      <td>-0.918820</td>\n",
       "      <td>-0.929592</td>\n",
       "      <td>-0.919092</td>\n",
       "      <td>-0.925997</td>\n",
       "      <td>-0.915954</td>\n",
       "      <td>-0.884163</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.903080</td>\n",
       "      <td>-0.988678</td>\n",
       "      <td>-0.885641</td>\n",
       "      <td>-0.932794</td>\n",
       "      <td>-0.918998</td>\n",
       "      <td>-0.812912</td>\n",
       "      <td>-0.903828</td>\n",
       "      <td>-0.915996</td>\n",
       "      <td>-0.859072</td>\n",
       "      <td>-0.912142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>-0.999982</td>\n",
       "      <td>-0.927003</td>\n",
       "      <td>-0.916459</td>\n",
       "      <td>-0.919193</td>\n",
       "      <td>-0.912465</td>\n",
       "      <td>-0.925605</td>\n",
       "      <td>-0.920740</td>\n",
       "      <td>-0.923770</td>\n",
       "      <td>-0.927946</td>\n",
       "      <td>-0.893598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.849464</td>\n",
       "      <td>-0.977486</td>\n",
       "      <td>-0.751189</td>\n",
       "      <td>-0.886827</td>\n",
       "      <td>-0.880077</td>\n",
       "      <td>-0.743810</td>\n",
       "      <td>-0.886731</td>\n",
       "      <td>-0.904454</td>\n",
       "      <td>-0.809486</td>\n",
       "      <td>-0.885961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>-0.999916</td>\n",
       "      <td>-0.955343</td>\n",
       "      <td>-0.757361</td>\n",
       "      <td>-0.765798</td>\n",
       "      <td>-0.886407</td>\n",
       "      <td>-0.864660</td>\n",
       "      <td>-0.830860</td>\n",
       "      <td>-0.922551</td>\n",
       "      <td>-0.877884</td>\n",
       "      <td>-0.838867</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.820735</td>\n",
       "      <td>-0.992869</td>\n",
       "      <td>-0.855935</td>\n",
       "      <td>-0.845368</td>\n",
       "      <td>-0.928452</td>\n",
       "      <td>-0.700598</td>\n",
       "      <td>-0.822742</td>\n",
       "      <td>-0.909202</td>\n",
       "      <td>-0.789041</td>\n",
       "      <td>-0.807226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>-0.999928</td>\n",
       "      <td>-0.945442</td>\n",
       "      <td>-0.900623</td>\n",
       "      <td>-0.893344</td>\n",
       "      <td>-0.905720</td>\n",
       "      <td>-0.908961</td>\n",
       "      <td>-0.904568</td>\n",
       "      <td>-0.928567</td>\n",
       "      <td>-0.910034</td>\n",
       "      <td>-0.884117</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.862728</td>\n",
       "      <td>-0.991900</td>\n",
       "      <td>-0.827677</td>\n",
       "      <td>-0.845263</td>\n",
       "      <td>-0.906191</td>\n",
       "      <td>-0.592484</td>\n",
       "      <td>-0.447326</td>\n",
       "      <td>-0.646946</td>\n",
       "      <td>-0.354136</td>\n",
       "      <td>-0.288791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        conv1  layer1.0.conv1  layer1.0.conv2  layer1.0.conv3  layer1.1.conv1  \\\n",
       "0   -0.999991       -0.936642       -0.934466       -0.935969       -0.904005   \n",
       "1   -0.999955       -0.963097       -0.884371       -0.890870       -0.906541   \n",
       "2   -0.999948       -0.959299       -0.862562       -0.866453       -0.903425   \n",
       "3   -0.999968       -0.952015       -0.918668       -0.916533       -0.912042   \n",
       "4   -0.999962       -0.942360       -0.892700       -0.889042       -0.909401   \n",
       "..        ...             ...             ...             ...             ...   \n",
       "245 -0.999974       -0.956059       -0.908246       -0.908545       -0.915434   \n",
       "246 -0.999966       -0.955644       -0.913804       -0.911800       -0.918820   \n",
       "247 -0.999982       -0.927003       -0.916459       -0.919193       -0.912465   \n",
       "248 -0.999916       -0.955343       -0.757361       -0.765798       -0.886407   \n",
       "249 -0.999928       -0.945442       -0.900623       -0.893344       -0.905720   \n",
       "\n",
       "     layer1.1.conv2  layer1.1.conv3  layer1.2.conv1  layer1.2.conv2  \\\n",
       "0         -0.935814       -0.931759       -0.918483       -0.911972   \n",
       "1         -0.925186       -0.910882       -0.925319       -0.903241   \n",
       "2         -0.888381       -0.885857       -0.918011       -0.901663   \n",
       "3         -0.931676       -0.923608       -0.928480       -0.914450   \n",
       "4         -0.898860       -0.882733       -0.924383       -0.911258   \n",
       "..              ...             ...             ...             ...   \n",
       "245       -0.922431       -0.911444       -0.918918       -0.906877   \n",
       "246       -0.929592       -0.919092       -0.925997       -0.915954   \n",
       "247       -0.925605       -0.920740       -0.923770       -0.927946   \n",
       "248       -0.864660       -0.830860       -0.922551       -0.877884   \n",
       "249       -0.908961       -0.904568       -0.928567       -0.910034   \n",
       "\n",
       "     layer1.2.conv3  ...  layer3.5.conv3  layer4.0.conv1  layer4.0.conv2  \\\n",
       "0         -0.890295  ...       -0.137930       -0.770316       -0.453550   \n",
       "1         -0.878101  ...       -0.920808       -0.990737       -0.896368   \n",
       "2         -0.846007  ...       -0.887986       -0.991643       -0.874940   \n",
       "3         -0.888074  ...       -0.214854       -0.879364       -0.609108   \n",
       "4         -0.859058  ...       -0.867012       -0.988206       -0.849169   \n",
       "..              ...  ...             ...             ...             ...   \n",
       "245       -0.870916  ...       -0.890650       -0.986374       -0.857403   \n",
       "246       -0.884163  ...       -0.903080       -0.988678       -0.885641   \n",
       "247       -0.893598  ...       -0.849464       -0.977486       -0.751189   \n",
       "248       -0.838867  ...       -0.820735       -0.992869       -0.855935   \n",
       "249       -0.884117  ...       -0.862728       -0.991900       -0.827677   \n",
       "\n",
       "     layer4.0.conv3  layer4.1.conv1  layer4.1.conv2  layer4.1.conv3  \\\n",
       "0         -0.456116       -0.594658       -0.359765       -0.178945   \n",
       "1         -0.936358       -0.929399       -0.823115       -0.915983   \n",
       "2         -0.924446       -0.919121       -0.789649       -0.890146   \n",
       "3         -0.485275       -0.752628       -0.663677       -0.454453   \n",
       "4         -0.868761       -0.908572       -0.765086       -0.820054   \n",
       "..              ...             ...             ...             ...   \n",
       "245       -0.919065       -0.909058       -0.791113       -0.905559   \n",
       "246       -0.932794       -0.918998       -0.812912       -0.903828   \n",
       "247       -0.886827       -0.880077       -0.743810       -0.886731   \n",
       "248       -0.845368       -0.928452       -0.700598       -0.822742   \n",
       "249       -0.845263       -0.906191       -0.592484       -0.447326   \n",
       "\n",
       "     layer4.2.conv1  layer4.2.conv2  layer4.2.conv3  \n",
       "0         -0.347093       -0.281436       -0.135682  \n",
       "1         -0.912726       -0.839880       -0.905891  \n",
       "2         -0.913709       -0.841277       -0.894605  \n",
       "3         -0.683164       -0.776617       -0.865294  \n",
       "4         -0.886940       -0.779974       -0.780867  \n",
       "..              ...             ...             ...  \n",
       "245       -0.905908       -0.812710       -0.891740  \n",
       "246       -0.915996       -0.859072       -0.912142  \n",
       "247       -0.904454       -0.809486       -0.885961  \n",
       "248       -0.909202       -0.789041       -0.807226  \n",
       "249       -0.646946       -0.354136       -0.288791  \n",
       "\n",
       "[250 rows x 49 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integration_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_integration_beauty(integration_ratings: pd.DataFrame, beauty_ratings: pd.DataFrame):\n",
    "    return integration_ratings.aggregate(lambda x: spearmanr(x, beauty_ratings)[0], axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv1            -0.109754\n",
       "layer1.0.conv1    0.107449\n",
       "layer1.0.conv2   -0.022921\n",
       "layer1.0.conv3   -0.041183\n",
       "layer1.1.conv1   -0.014913\n",
       "layer1.1.conv2   -0.045467\n",
       "layer1.1.conv3   -0.023069\n",
       "layer1.2.conv1   -0.030365\n",
       "layer1.2.conv2    0.019254\n",
       "layer1.2.conv3   -0.014621\n",
       "layer2.0.conv1   -0.024087\n",
       "layer2.0.conv2   -0.123377\n",
       "layer2.0.conv3   -0.085724\n",
       "layer2.1.conv1   -0.023309\n",
       "layer2.1.conv2    0.008717\n",
       "layer2.1.conv3    0.025263\n",
       "layer2.2.conv1    0.102302\n",
       "layer2.2.conv2    0.194538\n",
       "layer2.2.conv3    0.082341\n",
       "layer2.3.conv1    0.084602\n",
       "layer2.3.conv2    0.119133\n",
       "layer2.3.conv3    0.149158\n",
       "layer3.0.conv1    0.023309\n",
       "layer3.0.conv2    0.078263\n",
       "layer3.0.conv3   -0.038633\n",
       "layer3.1.conv1    0.083620\n",
       "layer3.1.conv2    0.004144\n",
       "layer3.1.conv3    0.042955\n",
       "layer3.2.conv1    0.025898\n",
       "layer3.2.conv2    0.014242\n",
       "layer3.2.conv3   -0.076855\n",
       "layer3.3.conv1    0.033383\n",
       "layer3.3.conv2    0.038585\n",
       "layer3.3.conv3         NaN\n",
       "layer3.4.conv1    0.031699\n",
       "layer3.4.conv2    0.029416\n",
       "layer3.4.conv3   -0.037299\n",
       "layer3.5.conv1    0.050597\n",
       "layer3.5.conv2    0.074062\n",
       "layer3.5.conv3    0.047134\n",
       "layer4.0.conv1    0.076207\n",
       "layer4.0.conv2    0.129475\n",
       "layer4.0.conv3    0.007922\n",
       "layer4.1.conv1    0.097228\n",
       "layer4.1.conv2    0.148081\n",
       "layer4.1.conv3    0.034989\n",
       "layer4.2.conv1    0.129601\n",
       "layer4.2.conv2    0.141113\n",
       "layer4.2.conv3    0.015181\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlate_integration_beauty(integration_ratings, data.beauty_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(object):\n",
    "    \"\"\"\n",
    "    Handles preparing images for input into activation extractors:\n",
    "        \n",
    "        - Load images (matlab arrays) from subfolder,\n",
    "            in alphanumerical order (corresponding to beauty ratings in file).\n",
    "        \n",
    "        - Transform into PyTorch format\n",
    "    \n",
    "    This class provides a iterator to do so.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, beauty_ratings_path=None):\n",
    "\n",
    "        dir_img_list    = list(f for f in os.listdir(os.path.join(img_dir, 'full')))\n",
    "        self.img_dir    = img_dir\n",
    "        self.img_list   = sorted(dir_img_list)\n",
    "        self.img_count  = len(dir_img_list)\n",
    "        if beauty_ratings_path is not None:\n",
    "            self.beauty_ratings = pd.read_csv(beauty_ratings_path, header=None).mean(axis=1)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.img_pos = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.img_pos < self.img_count:\n",
    "            # load arrays (transformed in matlab)\n",
    "            img_full = loadmat(os.path.join(self.img_dir,'full', self.img_list[self.img_pos]))[\"im\"]\n",
    "            img_v1 = loadmat(os.path.join(self.img_dir,'version1', self.img_list[self.img_pos]))[\"imv1\"]\n",
    "            img_v2 = loadmat(os.path.join(self.img_dir,'version2', self.img_list[self.img_pos]))[\"imv2\"]\n",
    "            \n",
    "            # convert to input format of Taskonomy models\n",
    "            img_full = torch.tensor(img_full).permute([2, 0, 1]).unsqueeze(0)\n",
    "            img_v1 = torch.tensor(img_v1).permute([2, 0, 1]).unsqueeze(0)\n",
    "            img_v2 = torch.tensor(img_v2).permute([2, 0, 1]).unsqueeze(0)\n",
    "            self.img_pos += 1\n",
    "            return img_full, img_v1, img_v2\n",
    "        else: # prepare for a possible next iteration\n",
    "            self.img_pos = 0\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Correlation (-Integration)\n",
    "def correlation_coeff(net, img_full, img_v1, img_v2):\n",
    "    \"\"\"Calculate correlation coefficient between full and average activation pattern\"\"\"    \n",
    "    # activations for full image and image parts\n",
    "    with torch.no_grad():\n",
    "        act_full, act_v1, act_v2 = net(img_full), net(img_v1), net(img_v2)\n",
    "\n",
    "    integration = {}\n",
    "    for (layer, act_full_, act_v1_, act_v2_) in zip(act_full.keys(), act_full.values(), act_v1.values(), act_v2.values()):\n",
    "        # average activation for image parts\n",
    "        act_avg_ = torch.stack((act_v1_, act_v2_), dim=0).mean(dim=0).flatten()\n",
    "        act_full_ = act_full_.flatten()\n",
    "        \n",
    "        integration[layer] = pearsonr(act_full_, act_avg_)[0]\n",
    "        if isnan(integration[layer]):\n",
    "            \n",
    "            globals().update({'af': act_full_, 'aa': act_avg_})\n",
    "            sys.exit(\"Error message\")\n",
    "        \n",
    "\n",
    "    return integration\n",
    "\n",
    "\n",
    "def calculate_dataset_correlation(ImageDataset_iterator, net):\n",
    "    \"\"\"Calculate integration for whole dataset\"\"\"\n",
    "    lst = []\n",
    "    for img_full, img_v1, img_v2 in ImageDataset_iterator:\n",
    "        lst.append(correlation_coeff(net, img_full, img_v1, img_v2))\n",
    "    \n",
    "    column_names = list(net(torch.zeros(1,3,256,256)).keys())\n",
    "    return pd.DataFrame(lst, columns=column_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'autoencoding'\n",
    "\n",
    "DATASET_NAMES = ('places1', 'places2', 'oasis')\n",
    "SCALE_NAMES = ('scale2','scale4','scale8','scale16','scale32')\n",
    "\n",
    "DATA_PATH = './data_256x256'\n",
    "BEHAVIOR_PATH = './behavior'\n",
    "RESULTS_PATH = './results_taskonomy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Error message",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Error message\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/miniconda3/envs/PytorchEnv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Import taskonomy model...\n",
    "VisualPriorRepresentation._load_unloaded_nets([model_name])\n",
    "net = VisualPriorRepresentation.feature_task_to_net[model_name]\n",
    "\n",
    "# ...and create activation extractor from it\n",
    "_, eval_nodes = get_graph_node_names(net)\n",
    "return_nodes = { node:node for node in eval_nodes if \"conv\" in node or 'fc' in node}\n",
    "activation_extractor = create_feature_extractor(net, return_nodes=return_nodes)\n",
    "\n",
    "dataset_name =  DATASET_NAMES[0]\n",
    "\n",
    "for scale_name in SCALE_NAMES:\n",
    "    \n",
    "    dataset = ImageDataset(\n",
    "        os.path.join(DATA_PATH, dataset_name, 'untransformed', scale_name))\n",
    "                \n",
    "    correlations = calculate_dataset_correlation(dataset, activation_extractor)\n",
    "    correlations.to_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'correlations.csv'), index=False, header=False)\n",
    "\n",
    "\n",
    "    #selfsimilarity = classes.calculate_dataset_self_similarity(dataset, activation_extractor)\n",
    "    #selfsimilarity.to_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'selfsimilarity.csv'), index=False, header=False)\n",
    "\n",
    "    #l2norm = classes.calculate_dataset_l2norm(dataset, activation_extractor)\n",
    "    #l2norm.to_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'l2norm.csv'), index=False, header=False)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "         0.0000e+00, -1.6382e-08])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
